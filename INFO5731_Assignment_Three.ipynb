{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DurgaBhavana/5731Submissions/blob/master/INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-hRx7xVvptu"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7lYZvstvtKQ"
      },
      "source": [
        "abstract_text = []\n",
        "count = 0\n",
        "for i in range(10):\n",
        "  link = 'https://citeseerx.ist.psu.edu/search;jsessionid=87FF6C66EA09F22314C131669600CF98?q=natural+language+processing&t=doc&sort=rlv&start='\n",
        "  page = requests.get(link + str (count))\n",
        "  abstract = (BeautifulSoup(page.text, 'html.parser')).find_all(class_='pubabstract')\n",
        "  count = count + 10\n",
        "  for i in abstract:\n",
        "    abstract_text.append(i.text.replace('\\n', '').strip())"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0rienzavvnm",
        "outputId": "a37bdc7d-b409-4113-f293-45707f5f3566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df = pd.DataFrame((abstract_text), columns =['Abstract'])\n",
        "df"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional random fields for natural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the issue of cooperation b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>This paper presents a workbench built by Pribe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abstract—Natural Language Processing (NLP) is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ABSTRACT: After twenty years of disfavor, a te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Text statistics are frequently used in stylome...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>We summarize our experience using FrameNet in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstract\n",
              "0                                  Abstract not found\n",
              "1   describe a method for statistical modeling bas...\n",
              "2   Scaling conditional random fields for natural ...\n",
              "3   The paper addresses the issue of cooperation b...\n",
              "4   In most natural language processing applicatio...\n",
              "..                                                ...\n",
              "95  This paper presents a workbench built by Pribe...\n",
              "96  Abstract—Natural Language Processing (NLP) is ...\n",
              "97  ABSTRACT: After twenty years of disfavor, a te...\n",
              "98  Text statistics are frequently used in stylome...\n",
              "99  We summarize our experience using FrameNet in ...\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJpIa1RvzJSw"
      },
      "source": [
        "#Text Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmxDWYy4xWzZ",
        "outputId": "0ee4b5fe-f028-4234-894b-32692ee4fad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMMvm9FRxaI8"
      },
      "source": [
        "# Special characters removal\n",
        "df['After noise removal'] = df['Abstract'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))\n",
        "# Punctuation removal\n",
        "df['Punctuation removal'] = df['After noise removal'].str.replace('[^\\w\\s]','')\n",
        "# Remove numbers\n",
        "df['Remove numbers'] = df['Punctuation removal'].str.replace('\\d+', '')\n",
        "# Stopwords removal\n",
        "stop_word = stopwords.words('english')\n",
        "df['Stopwords removal'] = df['Remove numbers'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_word))\n",
        "# Lower Casing\n",
        "df['Lower casing'] = df['Stopwords removal'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "# Stemming\n",
        "st = PorterStemmer()\n",
        "df['Stemming'] = df['Lower casing'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\n",
        "# Lemmatization\n",
        "df['Lemmatization'] = df['Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr1RfCNgBG3s"
      },
      "source": [
        "final_df = df"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vad14EaziQle"
      },
      "source": [
        "\n",
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on the dataset you collected from assignment two. You may use scipy and numpy package in this assignment.\n",
        "\n",
        "# **Question 1: Understand N-gram**\n",
        "\n",
        "\n",
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1DFaP1ZBGPK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDG6j8hBiWCZ",
        "outputId": "c0d3cdc1-113f-44b3-cf9b-bbeea7fd4822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1.1\n",
        "nltk.download('punkt')\n",
        "import itertools\n",
        "list_of_words = []\n",
        "for i in df['Lower casing']:\n",
        "  list_of_words.append(nltk.tokenize.word_tokenize(i))\n",
        "words = [i for i in list_of_words if i != []]\n",
        "looping = list(itertools.chain.from_iterable(words))\n",
        "#N-grams (N=3 -> Trigrams)\n",
        "fd = nltk.FreqDist(nltk.trigrams(looping))\n",
        "fd.most_common()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('natural', 'language', 'processing'), 76),\n",
              " (('language', 'processing', 'nlp'), 21),\n",
              " (('language', 'processing', 'systems'), 5),\n",
              " (('abstract', 'natural', 'language'), 4),\n",
              " (('sense', 'id', 'sense'), 4),\n",
              " (('language', 'processing', 'the'), 3),\n",
              " (('language', 'processing', 'we'), 3),\n",
              " (('language', 'processing', 'techniques'), 3),\n",
              " (('processing', 'nlp', 'tasks'), 3),\n",
              " (('lex', 'sign', 'sense'), 3),\n",
              " (('sign', 'sense', 'id'), 3),\n",
              " (('id', 'sense', 'id'), 3),\n",
              " (('in', 'natural', 'language'), 2),\n",
              " (('language', 'processing', 'applications'), 2),\n",
              " (('neural', 'network', 'architecture'), 2),\n",
              " (('language', 'processing', 'tasks'), 2),\n",
              " (('part', 'speech', 'tagging'), 2),\n",
              " (('using', 'natural', 'language'), 2),\n",
              " (('natural', 'languages', 'languages'), 2),\n",
              " (('languages', 'languages', 'spoken'), 2),\n",
              " (('statistical', 'natural', 'language'), 2),\n",
              " (('information', 'retrieval', 'system'), 2),\n",
              " (('in', 'paper', 'describe'), 2),\n",
              " (('processing', 'nlp', 'techniques'), 2),\n",
              " (('this', 'paper', 'reviews'), 2),\n",
              " (('processing', 'systems', 'the'), 2),\n",
              " (('recent', 'development', 'natural'), 2),\n",
              " (('development', 'natural', 'language'), 2),\n",
              " (('machine', 'learning', 'techniques'), 2),\n",
              " (('abstract', 'found', 'in'), 2),\n",
              " (('machine', 'learning', 'ml'), 2),\n",
              " (('abstract', 'found', 'abstract'), 2),\n",
              " (('parsing', 'word', 'sense'), 2),\n",
              " (('word', 'sense', 'disambiguation'), 2),\n",
              " (('state', 'art', 'plan'), 2),\n",
              " (('art', 'plan', 'recognition'), 2),\n",
              " (('plan', 'recognition', 'systems'), 2),\n",
              " (('recognition', 'systems', 'this'), 2),\n",
              " (('systems', 'this', 'paper'), 2),\n",
              " (('this', 'paper', 'outline'), 2),\n",
              " (('paper', 'outline', 'relations'), 2),\n",
              " (('outline', 'relations', 'natural'), 2),\n",
              " (('relations', 'natural', 'language'), 2),\n",
              " (('processing', 'nlp', 'plan'), 2),\n",
              " (('nlp', 'plan', 'recognition'), 2),\n",
              " (('plan', 'recognition', 'pr'), 2),\n",
              " (('recognition', 'pr', 'argue'), 2),\n",
              " (('pr', 'argue', 'effectively'), 2),\n",
              " (('argue', 'effectively', 'inform'), 2),\n",
              " (('effectively', 'inform', 'focus'), 2),\n",
              " (('inform', 'focus', 'key'), 2),\n",
              " (('focus', 'key', 'recent'), 2),\n",
              " (('key', 'recent', 'research'), 2),\n",
              " (('recent', 'research', 'results'), 2),\n",
              " (('research', 'results', 'nlp'), 2),\n",
              " (('results', 'nlp', 'argue'), 2),\n",
              " (('nlp', 'argue', 'applicability'), 2),\n",
              " (('argue', 'applicability', 'pr'), 2),\n",
              " (('language', 'processing', 'logic'), 2),\n",
              " (('processing', 'logic', 'programming'), 2),\n",
              " (('this', 'chapter', 'examines'), 2),\n",
              " (('chapter', 'examines', 'application'), 2),\n",
              " (('examines', 'application', 'natural'), 2),\n",
              " (('application', 'natural', 'language'), 2),\n",
              " (('language', 'processing', 'computerassisted'), 2),\n",
              " (('processing', 'computerassisted', 'language'), 2),\n",
              " (('computerassisted', 'language', 'learning'), 2),\n",
              " (('language', 'learning', 'including'), 2),\n",
              " (('learning', 'including', 'history'), 2),\n",
              " (('including', 'history', 'work'), 2),\n",
              " (('history', 'work', 'field'), 2),\n",
              " (('work', 'field', 'last'), 2),\n",
              " (('field', 'last', 'thirtyfive'), 2),\n",
              " (('last', 'thirtyfive', 'years'), 2),\n",
              " (('thirtyfive', 'years', 'focus'), 2),\n",
              " (('years', 'focus', 'current'), 2),\n",
              " (('focus', 'current', 'developments'), 2),\n",
              " (('current', 'developments', 'opportunities'), 2),\n",
              " (('this', 'paper', 'presents'), 2),\n",
              " (('abstract', 'found', 'describe'), 1),\n",
              " (('found', 'describe', 'method'), 1),\n",
              " (('describe', 'method', 'statistical'), 1),\n",
              " (('method', 'statistical', 'modeling'), 1),\n",
              " (('statistical', 'modeling', 'based'), 1),\n",
              " (('modeling', 'based', 'maximum'), 1),\n",
              " (('based', 'maximum', 'entropy'), 1),\n",
              " (('maximum', 'entropy', 'we'), 1),\n",
              " (('entropy', 'we', 'present'), 1),\n",
              " (('we', 'present', 'maximum'), 1),\n",
              " (('present', 'maximum', 'likelihood'), 1),\n",
              " (('maximum', 'likelihood', 'approach'), 1),\n",
              " (('likelihood', 'approach', 'automatically'), 1),\n",
              " (('approach', 'automatically', 'constructing'), 1),\n",
              " (('automatically', 'constructing', 'maximum'), 1),\n",
              " (('constructing', 'maximum', 'entropy'), 1),\n",
              " (('maximum', 'entropy', 'models'), 1),\n",
              " (('entropy', 'models', 'describe'), 1),\n",
              " (('models', 'describe', 'implement'), 1),\n",
              " (('describe', 'implement', 'approach'), 1),\n",
              " (('implement', 'approach', 'efficiently'), 1),\n",
              " (('approach', 'efficiently', 'using'), 1),\n",
              " (('efficiently', 'using', 'examples'), 1),\n",
              " (('using', 'examples', 'several'), 1),\n",
              " (('examples', 'several', 'problems'), 1),\n",
              " (('several', 'problems', 'natural'), 1),\n",
              " (('problems', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'scaling'), 1),\n",
              " (('processing', 'scaling', 'conditional'), 1),\n",
              " (('scaling', 'conditional', 'random'), 1),\n",
              " (('conditional', 'random', 'fields'), 1),\n",
              " (('random', 'fields', 'natural'), 1),\n",
              " (('fields', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'terms'), 1),\n",
              " (('processing', 'terms', 'conditions'), 1),\n",
              " (('terms', 'conditions', 'terms'), 1),\n",
              " (('conditions', 'terms', 'conditions'), 1),\n",
              " (('terms', 'conditions', 'copyright'), 1),\n",
              " (('conditions', 'copyright', 'works'), 1),\n",
              " (('copyright', 'works', 'deposited'), 1),\n",
              " (('works', 'deposited', 'minerva'), 1),\n",
              " (('deposited', 'minerva', 'access'), 1),\n",
              " (('minerva', 'access', 'retained'), 1),\n",
              " (('access', 'retained', 'the'), 1),\n",
              " (('retained', 'the', 'paper'), 1),\n",
              " (('the', 'paper', 'addresses'), 1),\n",
              " (('paper', 'addresses', 'issue'), 1),\n",
              " (('addresses', 'issue', 'cooperation'), 1),\n",
              " (('issue', 'cooperation', 'linguistics'), 1),\n",
              " (('cooperation', 'linguistics', 'natural'), 1),\n",
              " (('linguistics', 'natural', 'language'), 1),\n",
              " (('processing', 'nlp', 'general'), 1),\n",
              " (('nlp', 'general', 'linguistics'), 1),\n",
              " (('general', 'linguistics', 'machine'), 1),\n",
              " (('linguistics', 'machine', 'translation'), 1),\n",
              " (('machine', 'translation', 'mt'), 1),\n",
              " (('translation', 'mt', 'particular'), 1),\n",
              " (('mt', 'particular', 'it'), 1),\n",
              " (('particular', 'it', 'focuses'), 1),\n",
              " (('it', 'focuses', 'one'), 1),\n",
              " (('focuses', 'one', 'direction'), 1),\n",
              " (('one', 'direction', 'cooperation'), 1),\n",
              " (('direction', 'cooperation', 'namely'), 1),\n",
              " (('cooperation', 'namely', 'applications'), 1),\n",
              " (('namely', 'applications', 'linguistics'), 1),\n",
              " (('applications', 'linguistics', 'nlp'), 1),\n",
              " (('linguistics', 'nlp', 'virtually'), 1),\n",
              " (('nlp', 'virtually', 'in'), 1),\n",
              " (('virtually', 'in', 'natural'), 1),\n",
              " (('processing', 'applications', 'description'), 1),\n",
              " (('applications', 'description', 'logics'), 1),\n",
              " (('description', 'logics', 'used'), 1),\n",
              " (('logics', 'used', 'encode'), 1),\n",
              " (('used', 'encode', 'knowledge'), 1),\n",
              " (('encode', 'knowledge', 'base'), 1),\n",
              " (('knowledge', 'base', 'syntactic'), 1),\n",
              " (('base', 'syntactic', 'semantic'), 1),\n",
              " (('syntactic', 'semantic', 'pragmatic'), 1),\n",
              " (('semantic', 'pragmatic', 'elements'), 1),\n",
              " (('pragmatic', 'elements', 'needed'), 1),\n",
              " (('elements', 'needed', 'drive'), 1),\n",
              " (('needed', 'drive', 'semantic'), 1),\n",
              " (('drive', 'semantic', 'interpretation'), 1),\n",
              " (('semantic', 'interpretation', 'natural'), 1),\n",
              " (('interpretation', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'generation'), 1),\n",
              " (('language', 'generation', 'processes'), 1),\n",
              " (('generation', 'processes', 'more'), 1),\n",
              " (('processes', 'more', 'recently'), 1),\n",
              " (('more', 'recently', 'description'), 1),\n",
              " (('recently', 'description', 'logics'), 1),\n",
              " (('description', 'logics', 'we'), 1),\n",
              " (('logics', 'we', 'propose'), 1),\n",
              " (('we', 'propose', 'unified'), 1),\n",
              " (('propose', 'unified', 'neural'), 1),\n",
              " (('unified', 'neural', 'network'), 1),\n",
              " (('network', 'architecture', 'learning'), 1),\n",
              " (('architecture', 'learning', 'algorithm'), 1),\n",
              " (('learning', 'algorithm', 'applied'), 1),\n",
              " (('algorithm', 'applied', 'various'), 1),\n",
              " (('applied', 'various', 'natural'), 1),\n",
              " (('various', 'natural', 'language'), 1),\n",
              " (('processing', 'tasks', 'including'), 1),\n",
              " (('tasks', 'including', 'part'), 1),\n",
              " (('including', 'part', 'speech'), 1),\n",
              " (('speech', 'tagging', 'chunking'), 1),\n",
              " (('tagging', 'chunking', 'named'), 1),\n",
              " (('chunking', 'named', 'entity'), 1),\n",
              " (('named', 'entity', 'recognition'), 1),\n",
              " (('entity', 'recognition', 'semantic'), 1),\n",
              " (('recognition', 'semantic', 'role'), 1),\n",
              " (('semantic', 'role', 'labeling'), 1),\n",
              " (('role', 'labeling', 'this'), 1),\n",
              " (('labeling', 'this', 'versatility'), 1),\n",
              " (('this', 'versatility', 'achieved'), 1),\n",
              " (('versatility', 'achieved', 'trying'), 1),\n",
              " (('achieved', 'trying', 'avoid'), 1),\n",
              " (('trying', 'avoid', 'task'), 1),\n",
              " (('avoid', 'task', 'natural'), 1),\n",
              " (('task', 'natural', 'language'), 1),\n",
              " (('processing', 'the', 'subject'), 1),\n",
              " (('the', 'subject', 'natural'), 1),\n",
              " (('subject', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'considered'), 1),\n",
              " (('processing', 'considered', 'broad'), 1),\n",
              " (('considered', 'broad', 'narrow'), 1),\n",
              " (('broad', 'narrow', 'senses'), 1),\n",
              " (('narrow', 'senses', 'in'), 1),\n",
              " (('senses', 'in', 'broad'), 1),\n",
              " (('in', 'broad', 'sense'), 1),\n",
              " (('broad', 'sense', 'covers'), 1),\n",
              " (('sense', 'covers', 'processing'), 1),\n",
              " (('covers', 'processing', 'issues'), 1),\n",
              " (('processing', 'issues', 'levels'), 1),\n",
              " (('issues', 'levels', 'natural'), 1),\n",
              " (('levels', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'understanding'), 1),\n",
              " (('language', 'understanding', 'including'), 1),\n",
              " (('understanding', 'including', 'speech'), 1),\n",
              " (('including', 'speech', 'recognition'), 1),\n",
              " (('speech', 'recognition', 'syntactic'), 1),\n",
              " (('recognition', 'syntactic', 'semantic'), 1),\n",
              " (('syntactic', 'semantic', 'analysis'), 1),\n",
              " (('semantic', 'analysis', 'sentences'), 1),\n",
              " (('analysis', 'sentences', 'robots'), 1),\n",
              " (('sentences', 'robots', 'interact'), 1),\n",
              " (('robots', 'interact', 'humans'), 1),\n",
              " (('interact', 'humans', 'face'), 1),\n",
              " (('humans', 'face', 'face'), 1),\n",
              " (('face', 'face', 'using'), 1),\n",
              " (('face', 'using', 'natural'), 1),\n",
              " (('natural', 'language', 'need'), 1),\n",
              " (('language', 'need', 'responsive'), 1),\n",
              " (('need', 'responsive', 'way'), 1),\n",
              " (('responsive', 'way', 'humans'), 1),\n",
              " (('way', 'humans', 'use'), 1),\n",
              " (('humans', 'use', 'language'), 1),\n",
              " (('use', 'language', 'situations'), 1),\n",
              " (('language', 'situations', 'we'), 1),\n",
              " (('situations', 'we', 'propose'), 1),\n",
              " (('we', 'propose', 'psychologicallyinspired'), 1),\n",
              " (('propose', 'psychologicallyinspired', 'natural'), 1),\n",
              " (('psychologicallyinspired', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'system'), 1),\n",
              " (('processing', 'system', 'robots'), 1),\n",
              " (('system', 'robots', 'performs'), 1),\n",
              " (('robots', 'performs', 'incremental'), 1),\n",
              " (('performs', 'incremental', 'semantic'), 1),\n",
              " (('incremental', 'semantic', 'interpretation'), 1),\n",
              " (('semantic', 'interpretation', 'spoken'), 1),\n",
              " (('interpretation', 'spoken', 'utterances'), 1),\n",
              " (('spoken', 'utterances', 'natural'), 1),\n",
              " (('utterances', 'natural', 'languages'), 1),\n",
              " (('languages', 'spoken', 'humans'), 1),\n",
              " (('spoken', 'humans', 'currently'), 1),\n",
              " (('humans', 'currently', 'yet'), 1),\n",
              " (('currently', 'yet', 'point'), 1),\n",
              " (('yet', 'point', 'languages'), 1),\n",
              " (('point', 'languages', 'unprocessed'), 1),\n",
              " (('languages', 'unprocessed', 'forms'), 1),\n",
              " (('unprocessed', 'forms', 'understood'), 1),\n",
              " (('forms', 'understood', 'computers'), 1),\n",
              " (('understood', 'computers', 'natural'), 1),\n",
              " (('computers', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'collection'), 1),\n",
              " (('processing', 'collection', 'techniques'), 1),\n",
              " (('collection', 'techniques', 'employed'), 1),\n",
              " (('techniques', 'employed', 'try'), 1),\n",
              " (('employed', 'try', 'accomplish'), 1),\n",
              " (('try', 'accomplish', 'goal'), 1),\n",
              " (('accomplish', 'goal', 'the'), 1),\n",
              " (('goal', 'the', 'field'), 1),\n",
              " (('the', 'field', 'natural'), 1),\n",
              " (('field', 'natural', 'abstract'), 1),\n",
              " (('natural', 'abstract', 'ambiguity'), 1),\n",
              " (('abstract', 'ambiguity', 'referred'), 1),\n",
              " (('ambiguity', 'referred', 'ability'), 1),\n",
              " (('referred', 'ability', 'one'), 1),\n",
              " (('ability', 'one', 'meaning'), 1),\n",
              " (('one', 'meaning', 'understood'), 1),\n",
              " (('meaning', 'understood', 'one'), 1),\n",
              " (('understood', 'one', 'way'), 1),\n",
              " (('one', 'way', 'natural'), 1),\n",
              " (('way', 'natural', 'languages'), 1),\n",
              " (('natural', 'languages', 'ambiguous'), 1),\n",
              " (('languages', 'ambiguous', 'computers'), 1),\n",
              " (('ambiguous', 'computers', 'able'), 1),\n",
              " (('computers', 'able', 'understand'), 1),\n",
              " (('able', 'understand', 'language'), 1),\n",
              " (('understand', 'language', 'way'), 1),\n",
              " (('language', 'way', 'people'), 1),\n",
              " (('way', 'people', 'natural'), 1),\n",
              " (('people', 'natural', 'language'), 1),\n",
              " (('processing', 'nlp', 'concerned'), 1),\n",
              " (('nlp', 'concerned', 'development'), 1),\n",
              " (('concerned', 'development', 'introduction'), 1),\n",
              " (('development', 'introduction', 'statistical'), 1),\n",
              " (('introduction', 'statistical', 'natural'), 1),\n",
              " (('language', 'processing', 'snlp'), 1),\n",
              " (('processing', 'snlp', 'field'), 1),\n",
              " (('snlp', 'field', 'lying'), 1),\n",
              " (('field', 'lying', 'intersection'), 1),\n",
              " (('lying', 'intersection', 'natural'), 1),\n",
              " (('intersection', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'machine'), 1),\n",
              " (('processing', 'machine', 'learning'), 1),\n",
              " (('machine', 'learning', 'snlp'), 1),\n",
              " (('learning', 'snlp', 'di'), 1),\n",
              " (('snlp', 'di', 'ers'), 1),\n",
              " (('di', 'ers', 'traditional'), 1),\n",
              " (('ers', 'traditional', 'natural'), 1),\n",
              " (('traditional', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'instead'), 1),\n",
              " (('processing', 'instead', 'linguist'), 1),\n",
              " (('instead', 'linguist', 'manually'), 1),\n",
              " (('linguist', 'manually', 'construct'), 1),\n",
              " (('manually', 'construct', 'model'), 1),\n",
              " (('construct', 'model', 'given'), 1),\n",
              " (('model', 'given', 'linguistic'), 1),\n",
              " (('given', 'linguistic', 'text'), 1),\n",
              " (('linguistic', 'text', 'directly'), 1),\n",
              " (('text', 'directly', 'rather'), 1),\n",
              " (('directly', 'rather', 'e'), 1),\n",
              " (('rather', 'e', 'g'), 1),\n",
              " (('e', 'g', 'titles'), 1),\n",
              " (('g', 'titles', 'abstracts'), 1),\n",
              " (('titles', 'abstracts', 'suggests'), 1),\n",
              " (('abstracts', 'suggests', 'appropriate'), 1),\n",
              " (('suggests', 'appropriate', 'approaches'), 1),\n",
              " (('appropriate', 'approaches', 'focus'), 1),\n",
              " (('approaches', 'focus', 'role'), 1),\n",
              " (('focus', 'role', 'natural'), 1),\n",
              " (('role', 'natural', 'language'), 1),\n",
              " (('processing', 'the', 'paper'), 1),\n",
              " (('the', 'paper', 'also'), 1),\n",
              " (('paper', 'also', 'comments'), 1),\n",
              " (('also', 'comments', 'possible'), 1),\n",
              " (('comments', 'possible', 'connections'), 1),\n",
              " (('possible', 'connections', 'data'), 1),\n",
              " (('connections', 'data', 'knowledge'), 1),\n",
              " (('data', 'knowledge', 'retrieval'), 1),\n",
              " (('knowledge', 'retrieval', 'concludes'), 1),\n",
              " (('retrieval', 'concludes', 'emphasizing'), 1),\n",
              " (('concludes', 'emphasizing', 'importance'), 1),\n",
              " (('emphasizing', 'importance', 'rigorous'), 1),\n",
              " (('importance', 'rigorous', 'abstract'), 1),\n",
              " (('rigorous', 'abstract', 'language'), 1),\n",
              " (('abstract', 'language', 'way'), 1),\n",
              " (('language', 'way', 'communicating'), 1),\n",
              " (('way', 'communicating', 'words'), 1),\n",
              " (('communicating', 'words', 'language'), 1),\n",
              " (('words', 'language', 'helps'), 1),\n",
              " (('language', 'helps', 'understanding'), 1),\n",
              " (('helps', 'understanding', 'world'), 1),\n",
              " (('understanding', 'world', 'get'), 1),\n",
              " (('world', 'get', 'better'), 1),\n",
              " (('get', 'better', 'insight'), 1),\n",
              " (('better', 'insight', 'world'), 1),\n",
              " (('insight', 'world', 'language'), 1),\n",
              " (('world', 'language', 'helps'), 1),\n",
              " (('language', 'helps', 'speakers'), 1),\n",
              " (('helps', 'speakers', 'vague'), 1),\n",
              " (('speakers', 'vague', 'precise'), 1),\n",
              " (('vague', 'precise', 'like'), 1),\n",
              " (('precise', 'like', 'nlp'), 1),\n",
              " (('like', 'nlp', 'stands'), 1),\n",
              " (('nlp', 'stands', 'natural'), 1),\n",
              " (('stands', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'natural'), 1),\n",
              " (('processing', 'natural', 'languages'), 1),\n",
              " (('languages', 'spoken', 'we'), 1),\n",
              " (('spoken', 'we', 'report'), 1),\n",
              " (('we', 'report', 'experiments'), 1),\n",
              " (('report', 'experiments', 'use'), 1),\n",
              " (('experiments', 'use', 'standard'), 1),\n",
              " (('use', 'standard', 'natural'), 1),\n",
              " (('standard', 'natural', 'language'), 1),\n",
              " (('processing', 'nlp', 'tools'), 1),\n",
              " (('nlp', 'tools', 'analysis'), 1),\n",
              " (('tools', 'analysis', 'music'), 1),\n",
              " (('analysis', 'music', 'lyrics'), 1),\n",
              " (('music', 'lyrics', 'a'), 1),\n",
              " (('lyrics', 'a', 'significant'), 1),\n",
              " (('a', 'significant', 'amount'), 1),\n",
              " (('significant', 'amount', 'music'), 1),\n",
              " (('amount', 'music', 'audio'), 1),\n",
              " (('music', 'audio', 'lyrics'), 1),\n",
              " (('audio', 'lyrics', 'lyrics'), 1),\n",
              " (('lyrics', 'lyrics', 'encode'), 1),\n",
              " (('lyrics', 'encode', 'important'), 1),\n",
              " (('encode', 'important', 'part'), 1),\n",
              " (('important', 'part', 'semantics'), 1),\n",
              " (('part', 'semantics', 'song'), 1),\n",
              " (('semantics', 'song', 'therefore'), 1),\n",
              " (('song', 'therefore', 'analysis'), 1),\n",
              " (('therefore', 'analysis', 'complements'), 1),\n",
              " (('analysis', 'complements', 'acoustic'), 1),\n",
              " (('complements', 'acoustic', 'cultural'), 1),\n",
              " (('acoustic', 'cultural', 'paper'), 1),\n",
              " (('cultural', 'paper', 'describe'), 1),\n",
              " (('paper', 'describe', 'simple'), 1),\n",
              " (('describe', 'simple', 'rule'), 1),\n",
              " (('simple', 'rule', 'based'), 1),\n",
              " (('rule', 'based', 'approach'), 1),\n",
              " (('based', 'approach', 'automated'), 1),\n",
              " (('approach', 'automated', 'learning'), 1),\n",
              " (('automated', 'learning', 'linguistic'), 1),\n",
              " (('learning', 'linguistic', 'knowledge'), 1),\n",
              " (('linguistic', 'knowledge', 'this'), 1),\n",
              " (('knowledge', 'this', 'approach'), 1),\n",
              " (('this', 'approach', 'shown'), 1),\n",
              " (('approach', 'shown', 'number'), 1),\n",
              " (('shown', 'number', 'tasks'), 1),\n",
              " (('number', 'tasks', 'capture'), 1),\n",
              " (('tasks', 'capture', 'information'), 1),\n",
              " (('capture', 'information', 'clearer'), 1),\n",
              " (('information', 'clearer', 'direct'), 1),\n",
              " (('clearer', 'direct', 'fashion'), 1),\n",
              " (('direct', 'fashion', 'without'), 1),\n",
              " (('fashion', 'without', 'compromise'), 1),\n",
              " (('without', 'compromise', 'performance'), 1),\n",
              " (('compromise', 'performance', 'we'), 1),\n",
              " (('performance', 'we', 'present'), 1),\n",
              " (('we', 'present', 'detailed'), 1),\n",
              " (('present', 'detailed', 'case'), 1),\n",
              " (('detailed', 'case', 'study'), 1),\n",
              " (('case', 'study', 'learning'), 1),\n",
              " (('study', 'learning', 'method'), 1),\n",
              " (('learning', 'method', 'applied'), 1),\n",
              " (('method', 'applied', 'part'), 1),\n",
              " (('applied', 'part', 'speech'), 1),\n",
              " (('speech', 'tagging', 'this'), 1),\n",
              " (('tagging', 'this', 'paper'), 1),\n",
              " (('this', 'paper', 'focuses'), 1),\n",
              " (('paper', 'focuses', 'connectionist'), 1),\n",
              " (('focuses', 'connectionist', 'models'), 1),\n",
              " (('connectionist', 'models', 'natural'), 1),\n",
              " (('models', 'natural', 'language'), 1),\n",
              " (('processing', 'we', 'briefly'), 1),\n",
              " (('we', 'briefly', 'present'), 1),\n",
              " (('briefly', 'present', 'discuss'), 1),\n",
              " (('present', 'discuss', 'several'), 1),\n",
              " (('discuss', 'several', 'aspects'), 1),\n",
              " (('several', 'aspects', 'high'), 1),\n",
              " (('aspects', 'high', 'level'), 1),\n",
              " (('high', 'level', 'tasks'), 1),\n",
              " (('level', 'tasks', 'recently'), 1),\n",
              " (('tasks', 'recently', 'approached'), 1),\n",
              " (('recently', 'approached', 'connectionism'), 1),\n",
              " (('approached', 'connectionism', 'either'), 1),\n",
              " (('connectionism', 'either', 'localist'), 1),\n",
              " (('either', 'localist', 'parallel'), 1),\n",
              " (('localist', 'parallel', 'distributed'), 1),\n",
              " (('parallel', 'distributed', 'processing'), 1),\n",
              " (('distributed', 'processing', 'models'), 1),\n",
              " (('processing', 'models', 'several'), 1),\n",
              " (('models', 'several', 'interesting'), 1),\n",
              " (('several', 'interesting', 'architectures'), 1),\n",
              " (('interesting', 'architectures', 'process'), 1),\n",
              " (('architectures', 'process', 'language'), 1),\n",
              " (('process', 'language', 'understanding'), 1),\n",
              " (('language', 'understanding', 'this'), 1),\n",
              " (('understanding', 'this', 'new'), 1),\n",
              " (('this', 'new', 'approach'), 1),\n",
              " (('new', 'approach', 'natural'), 1),\n",
              " (('approach', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'based'), 1),\n",
              " (('processing', 'based', 'deterministic'), 1),\n",
              " (('based', 'deterministic', 'chaotic'), 1),\n",
              " (('deterministic', 'chaotic', 'behavior'), 1),\n",
              " (('chaotic', 'behavior', 'dynamical'), 1),\n",
              " (('behavior', 'dynamical', 'systems'), 1),\n",
              " (('dynamical', 'systems', 'paper'), 1),\n",
              " (('systems', 'paper', 'see'), 1),\n",
              " (('paper', 'see', 'schank'), 1),\n",
              " (('see', 'schank', 'theoretical'), 1),\n",
              " (('schank', 'theoretical', 'discussion'), 1),\n",
              " (('theoretical', 'discussion', 'kass'), 1),\n",
              " (('discussion', 'kass', 'leake'), 1),\n",
              " (('kass', 'leake', 'owens'), 1),\n",
              " (('leake', 'owens', 'brief'), 1),\n",
              " (('owens', 'brief', 'discussions'), 1),\n",
              " (('brief', 'discussions', 'program'), 1),\n",
              " (('discussions', 'program', 'built'), 1),\n",
              " (('program', 'built', 'around'), 1),\n",
              " (('built', 'around', 'principles'), 1),\n",
              " (('around', 'principles', 'goal'), 1),\n",
              " (('principles', 'goal', 'simply'), 1),\n",
              " (('goal', 'simply', 'point'), 1),\n",
              " (('simply', 'point', 'interest'), 1),\n",
              " (('point', 'interest', 'natural'), 1),\n",
              " (('interest', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'led'), 1),\n",
              " (('processing', 'led', 'us'), 1),\n",
              " (('led', 'us', 'naturally'), 1),\n",
              " (('us', 'naturally', 'indeed'), 1),\n",
              " (('naturally', 'indeed', 'inevitably'), 1),\n",
              " (('indeed', 'inevitably', 'objectives'), 1),\n",
              " (('inevitably', 'objectives', 'to'), 1),\n",
              " (('objectives', 'to', 'provide'), 1),\n",
              " (('to', 'provide', 'overview'), 1),\n",
              " (('provide', 'overview', 'tutorial'), 1),\n",
              " (('overview', 'tutorial', 'natural'), 1),\n",
              " (('tutorial', 'natural', 'language'), 1),\n",
              " (('processing', 'nlp', 'modern'), 1),\n",
              " (('nlp', 'modern', 'nlp'), 1),\n",
              " (('modern', 'nlp', 'system'), 1),\n",
              " (('nlp', 'system', 'design'), 1),\n",
              " (('system', 'design', 'target'), 1),\n",
              " (('design', 'target', 'audience'), 1),\n",
              " (('target', 'audience', 'this'), 1),\n",
              " (('audience', 'this', 'tutorial'), 1),\n",
              " (('this', 'tutorial', 'targets'), 1),\n",
              " (('tutorial', 'targets', 'medical'), 1),\n",
              " (('targets', 'medical', 'informatics'), 1),\n",
              " (('medical', 'informatics', 'generalist'), 1),\n",
              " (('informatics', 'generalist', 'limited'), 1),\n",
              " (('generalist', 'limited', 'acquaintance'), 1),\n",
              " (('limited', 'acquaintance', 'principles'), 1),\n",
              " (('acquaintance', 'principles', 'behind'), 1),\n",
              " (('principles', 'behind', 'nlp'), 1),\n",
              " (('behind', 'nlp', 'limited'), 1),\n",
              " (('nlp', 'limited', 'knowledge'), 1),\n",
              " (('limited', 'knowledge', 'current'), 1),\n",
              " (('knowledge', 'current', 'state'), 1),\n",
              " (('current', 'state', 'this'), 1),\n",
              " (('state', 'this', 'paper'), 1),\n",
              " (('this', 'paper', 'briefly'), 1),\n",
              " (('paper', 'briefly', 'describes'), 1),\n",
              " (('briefly', 'describes', 'current'), 1),\n",
              " (('describes', 'current', 'implementation'), 1),\n",
              " (('current', 'implementation', 'status'), 1),\n",
              " (('implementation', 'status', 'intelligent'), 1),\n",
              " (('status', 'intelligent', 'information'), 1),\n",
              " (('intelligent', 'information', 'retrieval'), 1),\n",
              " (('retrieval', 'system', 'marie'), 1),\n",
              " (('system', 'marie', 'employs'), 1),\n",
              " (('marie', 'employs', 'natural'), 1),\n",
              " (('employs', 'natural', 'language'), 1),\n",
              " (('processing', 'techniques', 'descriptive'), 1),\n",
              " (('techniques', 'descriptive', 'captions'), 1),\n",
              " (('descriptive', 'captions', 'used'), 1),\n",
              " (('captions', 'used', 'iden'), 1),\n",
              " (('used', 'iden', 'tify'), 1),\n",
              " (('iden', 'tify', 'photographic'), 1),\n",
              " (('tify', 'photographic', 'images'), 1),\n",
              " (('photographic', 'images', 'concerning'), 1),\n",
              " (('images', 'concerning', 'various'), 1),\n",
              " (('concerning', 'various', 'military'), 1),\n",
              " (('various', 'military', 'projects'), 1),\n",
              " (('military', 'projects', 'the'), 1),\n",
              " (('projects', 'the', 'captions'), 1),\n",
              " (('the', 'captions', 'parsed'), 1),\n",
              " (('captions', 'parsed', 'based'), 1),\n",
              " (('parsed', 'based', 'literature'), 1),\n",
              " (('based', 'literature', 'resources'), 1),\n",
              " (('literature', 'resources', 'we'), 1),\n",
              " (('resources', 'we', 'describe'), 1),\n",
              " (('we', 'describe', 'system'), 1),\n",
              " (('describe', 'system', 'agent'), 1),\n",
              " (('system', 'agent', 'directed'), 1),\n",
              " (('agent', 'directed', 'natural'), 1),\n",
              " (('directed', 'natural', 'language'), 1),\n",
              " (('language', 'processing', 'extract'), 1),\n",
              " (('processing', 'extract', 'information'), 1),\n",
              " (('extract', 'information', 'journal'), 1),\n",
              " (('information', 'journal', 'articles'), 1),\n",
              " (('journal', 'articles', 'an'), 1),\n",
              " (('articles', 'an', 'interface'), 1),\n",
              " (('an', 'interface', 'developed'), 1),\n",
              " (('interface', 'developed', 'permit'), 1),\n",
              " (('developed', 'permit', 'curation'), 1),\n",
              " (('permit', 'curation', 'nlp'), 1),\n",
              " (('curation', 'nlp', 'results'), 1),\n",
              " (('nlp', 'results', 'deposition'), 1),\n",
              " (('results', 'deposition', 'accepted'), 1),\n",
              " (('deposition', 'accepted', 'results'), 1),\n",
              " (('accepted', 'results', 'knowledge'), 1),\n",
              " (('results', 'knowledge', 'base'), 1),\n",
              " (('knowledge', 'base', 'motivation'), 1),\n",
              " (('base', 'motivation', 'the'), 1),\n",
              " (('motivation', 'the', 'advent'), 1),\n",
              " (('the', 'advent', 'high'), 1),\n",
              " (('advent', 'high', 'evaluation'), 1),\n",
              " (('high', 'evaluation', 'speech'), 1),\n",
              " (('evaluation', 'speech', 'processing'), 1),\n",
              " (('speech', 'processing', 'part'), 1),\n",
              " (('processing', 'part', 'surveys'), 1),\n",
              " (('part', 'surveys', 'significant'), 1),\n",
              " (('surveys', 'significant', 'evaluation'), 1),\n",
              " (('significant', 'evaluation', 'work'), 1),\n",
              " (('evaluation', 'work', 'done'), 1),\n",
              " (('work', 'done', 'far'), 1),\n",
              " (('done', 'far', 'instance'), 1),\n",
              " (('far', 'instance', 'machine'), 1),\n",
              " (('instance', 'machine', 'translation'), 1),\n",
              " (('machine', 'translation', 'discusses'), 1),\n",
              " (('translation', 'discusses', 'particular'), 1),\n",
              " (('discusses', 'particular', 'problems'), 1),\n",
              " (('particular', 'problems', 'generic'), 1),\n",
              " (('problems', 'generic', 'system'), 1),\n",
              " (('generic', 'system', 'evaluation'), 1),\n",
              " (('system', 'evaluation', 'the'), 1),\n",
              " (('evaluation', 'the', 'conclusion'), 1),\n",
              " (('the', 'conclusion', 'evaluation'), 1),\n",
              " (('conclusion', 'evaluation', 'strategies'), 1),\n",
              " (('evaluation', 'strategies', 'techniques'), 1),\n",
              " (('strategies', 'techniques', 'nlp'), 1),\n",
              " (('techniques', 'nlp', 'need'), 1),\n",
              " (('nlp', 'need', 'much'), 1),\n",
              " (('need', 'much', 'development'), 1),\n",
              " (('much', 'development', 'particular'), 1),\n",
              " (('development', 'particular', 'similar'), 1),\n",
              " (('particular', 'similar', 'way'), 1),\n",
              " (('similar', 'way', 'humans'), 1),\n",
              " (('way', 'humans', 'intuitively'), 1),\n",
              " (('humans', 'intuitively', 'order'), 1),\n",
              " (('intuitively', 'order', 'eliminate'), 1),\n",
              " (('order', 'eliminate', 'noisy'), 1),\n",
              " (('eliminate', 'noisy', 'content'), 1),\n",
              " (('noisy', 'content', 'in'), 1),\n",
              " (('content', 'in', 'paper'), 1),\n",
              " (('paper', 'describe', 'combination'), 1),\n",
              " (('describe', 'combination', 'html'), 1),\n",
              " (('combination', 'html', 'dom'), 1),\n",
              " (('html', 'dom', 'analysis'), 1),\n",
              " (('dom', 'analysis', 'natural'), 1),\n",
              " (('analysis', 'natural', 'language'), 1),\n",
              " (('nlp', 'techniques', 'automated'), 1),\n",
              " (('techniques', 'automated', 'extractions'), 1),\n",
              " (('automated', 'extractions', 'main'), 1),\n",
              " (('extractions', 'main', 'article'), 1),\n",
              " (('main', 'article', 'associated'), 1),\n",
              " (('article', 'associated', 'images'), 1),\n",
              " (('associated', 'images', 'web'), 1),\n",
              " (('images', 'web', 'pages'), 1),\n",
              " (('web', 'pages', 'abstract'), 1),\n",
              " (('pages', 'abstract', 'natural'), 1),\n",
              " (('language', 'processing', 'theoretically'), 1),\n",
              " (('processing', 'theoretically', 'motivated'), 1),\n",
              " (('theoretically', 'motivated', 'range'), 1),\n",
              " (('motivated', 'range', 'computational'), 1),\n",
              " (('range', 'computational', 'techniques'), 1),\n",
              " (('computational', 'techniques', 'analysing'), 1),\n",
              " (('techniques', 'analysing', 'representing'), 1),\n",
              " (('analysing', 'representing', 'naturally'), 1),\n",
              " (('representing', 'naturally', 'occurring'), 1),\n",
              " (('naturally', 'occurring', 'texts'), 1),\n",
              " (('occurring', 'texts', 'one'), 1),\n",
              " (('texts', 'one', 'levels'), 1),\n",
              " (('one', 'levels', 'linguistic'), 1),\n",
              " (('levels', 'linguistic', 'analysis'), 1),\n",
              " (('linguistic', 'analysis', 'purpose'), 1),\n",
              " (('analysis', 'purpose', 'achieving'), 1),\n",
              " (('purpose', 'achieving', 'human'), 1),\n",
              " (('achieving', 'human', 'like'), 1),\n",
              " (('human', 'like', 'language'), 1),\n",
              " (('like', 'language', 'processing'), 1),\n",
              " (('language', 'processing', 'range'), 1),\n",
              " (('processing', 'range', 'tasks'), 1),\n",
              " (('range', 'tasks', 'this'), 1),\n",
              " (('tasks', 'this', 'paper'), 1),\n",
              " (('paper', 'reviews', 'processes'), 1),\n",
              " (('reviews', 'processes', 'involved'), 1),\n",
              " (('processes', 'involved', 'natural'), 1),\n",
              " (('involved', 'natural', 'language'), 1),\n",
              " (('processing', 'nlp', 'it'), 1),\n",
              " (('nlp', 'it', 'demonstrates'), 1),\n",
              " (('it', 'demonstrates', 'various'), 1),\n",
              " (('demonstrates', 'various', 'kinds'), 1),\n",
              " (('various', 'kinds', 'choices'), 1),\n",
              " (('kinds', 'choices', 'need'), 1),\n",
              " (('choices', 'need', 'taken'), 1),\n",
              " (('need', 'taken', 'execution'), 1),\n",
              " (('taken', 'execution', 'word'), 1),\n",
              " (('execution', 'word', 'morphology'), 1),\n",
              " (('word', 'morphology', 'syntactic'), 1),\n",
              " (('morphology', 'syntactic', 'text'), 1),\n",
              " (('syntactic', 'text', 'analysis'), 1),\n",
              " (('text', 'analysis', 'text'), 1),\n",
              " (('analysis', 'text', 'generation'), 1),\n",
              " (('text', 'generation', 'components'), 1),\n",
              " (('generation', 'components', 'it'), 1),\n",
              " (('components', 'it', 'compares'), 1),\n",
              " (('it', 'compares', 'time'), 1),\n",
              " (('compares', 'time', 'complexity'), 1),\n",
              " (('time', 'complexity', 'this'), 1),\n",
              " (('complexity', 'this', 'article'), 1),\n",
              " (('this', 'article', 'focusses'), 1),\n",
              " (('article', 'focusses', 'derivation'), 1),\n",
              " (('focusses', 'derivation', 'large'), 1),\n",
              " (('derivation', 'large', 'lexicons'), 1),\n",
              " (('large', 'lexicons', 'natural'), 1),\n",
              " (('lexicons', 'natural', 'language'), 1),\n",
              " (('processing', 'we', 'describe'), 1),\n",
              " (('we', 'describe', 'development'), 1),\n",
              " (('describe', 'development', 'dictionary'), 1),\n",
              " (('development', 'dictionary', 'support'), 1),\n",
              " (('dictionary', 'support', 'environment'), 1),\n",
              " (('support', 'environment', 'linking'), 1),\n",
              " (('environment', 'linking', 'restructured'), 1),\n",
              " (('linking', 'restructured', 'version'), 1),\n",
              " (('restructured', 'version', 'longman'), 1),\n",
              " (('version', 'longman', 'dictionary'), 1),\n",
              " (('longman', 'dictionary', 'contemporary'), 1),\n",
              " (('dictionary', 'contemporary', 'english'), 1),\n",
              " (('contemporary', 'english', 'natural'), 1),\n",
              " (('english', 'natural', 'language'), 1),\n",
              " (('systems', 'the', 'process'), 1),\n",
              " (('the', 'process', 'we'), 1),\n",
              " (('process', 'we', 'introduce'), 1),\n",
              " (('we', 'introduce', 'method'), 1),\n",
              " (('introduce', 'method', 'analyzing'), 1),\n",
              " (('method', 'analyzing', 'complexity'), 1),\n",
              " (('analyzing', 'complexity', 'natural'), 1),\n",
              " (('complexity', 'natural', 'language'), 1),\n",
              " (('processing', 'tasks', 'predicting'), 1),\n",
              " (('tasks', 'predicting', 'difficulty'), 1),\n",
              " (('predicting', 'difficulty', 'new'), 1),\n",
              " (('difficulty', 'new', 'nlp'), 1),\n",
              " (('new', 'nlp', 'tasks'), 1),\n",
              " (('nlp', 'tasks', 'our'), 1),\n",
              " (('tasks', 'our', 'complexity'), 1),\n",
              " (('our', 'complexity', 'measures'), 1),\n",
              " (('complexity', 'measures', 'derived'), 1),\n",
              " (('measures', 'derived', 'kolmogorov'), 1),\n",
              " (('derived', 'kolmogorov', 'complexity'), 1),\n",
              " (('kolmogorov', 'complexity', 'class'), 1),\n",
              " (('complexity', 'class', 'automata'), 1),\n",
              " (('class', 'automata', 'meaning'), 1),\n",
              " (('automata', 'meaning', 'automata'), 1),\n",
              " (('meaning', 'automata', 'whose'), 1),\n",
              " (('automata', 'whose', 'purpose'), 1),\n",
              " (('whose', 'purpose', 'extract'), 1),\n",
              " (('purpose', 'extract', 'relevant'), 1),\n",
              " (('extract', 'relevant', 'pieces'), 1),\n",
              " (('relevant', 'pieces', 'sounds'), 1),\n",
              " (('pieces', 'sounds', 'text'), 1),\n",
              " (('sounds', 'text', 'motion'), 1),\n",
              " (('text', 'motion', 'the'), 1),\n",
              " (('motion', 'the', 'techniques'), 1),\n",
              " (('the', 'techniques', 'developed'), 1),\n",
              " (('techniques', 'developed', 'deep'), 1),\n",
              " (('developed', 'deep', 'learning'), 1),\n",
              " (('deep', 'learning', 'research'), 1),\n",
              " (('learning', 'research', 'already'), 1),\n",
              " (('research', 'already', 'impacting'), 1),\n",
              " (('already', 'impacting', 'research'), 1),\n",
              " (('impacting', 'research', 'natural'), 1),\n",
              " (('research', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'process'), 1),\n",
              " (('language', 'process', 'this'), 1),\n",
              " (('process', 'this', 'paper'), 1),\n",
              " (('paper', 'reviews', 'recent'), 1),\n",
              " (('reviews', 'recent', 'research'), 1),\n",
              " (('recent', 'research', 'deep'), 1),\n",
              " (('research', 'deep', 'learning'), 1),\n",
              " (('deep', 'learning', 'applications'), 1),\n",
              " (('learning', 'applications', 'recent'), 1),\n",
              " (('applications', 'recent', 'development'), 1),\n",
              " (('language', 'processing', 'this'), 1),\n",
              " (('processing', 'this', 'author'), 1),\n",
              " (('this', 'author', 'produced'), 1),\n",
              " (('author', 'produced', 'version'), 1),\n",
              " (('produced', 'version', 'paper'), 1),\n",
              " (('version', 'paper', 'published'), 1),\n",
              " (('paper', 'published', 'the'), 1),\n",
              " (('published', 'the', 'abstract'), 1),\n",
              " (('the', 'abstract', 'natural'), 1),\n",
              " (('processing', 'nlp', 'application'), 1),\n",
              " (('nlp', 'application', 'automated'), 1),\n",
              " (('application', 'automated', 'parsing'), 1),\n",
              " (('automated', 'parsing', 'machine'), 1),\n",
              " (('parsing', 'machine', 'learning'), 1),\n",
              " (('learning', 'techniques', 'analyze'), 1),\n",
              " (('techniques', 'analyze', 'standard'), 1),\n",
              " (('analyze', 'standard', 'text'), 1),\n",
              " (('standard', 'text', 'applications'), 1),\n",
              " (('text', 'applications', 'nlp'), 1),\n",
              " (('applications', 'nlp', 'requirements'), 1),\n",
              " (('nlp', 'requirements', 'engineering'), 1),\n",
              " (('requirements', 'engineering', 'include'), 1),\n",
              " (('engineering', 'include', 'extraction'), 1),\n",
              " (('include', 'extraction', 'ontologies'), 1),\n",
              " (('extraction', 'ontologies', 'requirements'), 1),\n",
              " (('ontologies', 'requirements', 'specification'), 1),\n",
              " (('requirements', 'specification', 'use'), 1),\n",
              " (('specification', 'use', 'nlp'), 1),\n",
              " (('use', 'nlp', 'verify'), 1),\n",
              " (('nlp', 'verify', 'consistency'), 1),\n",
              " (('verify', 'consistency', 'statistical'), 1),\n",
              " (('consistency', 'statistical', 'baseline'), 1),\n",
              " (('statistical', 'baseline', 'including'), 1),\n",
              " (('baseline', 'including', 'forgiving'), 1),\n",
              " (('including', 'forgiving', 'nature'), 1),\n",
              " (('forgiving', 'nature', 'broad'), 1),\n",
              " (('nature', 'broad', 'coverage'), 1),\n",
              " (('broad', 'coverage', 'typical'), 1),\n",
              " (('coverage', 'typical', 'retrieval'), 1),\n",
              " (('typical', 'retrieval', 'task'), 1),\n",
              " (('retrieval', 'task', 'lack'), 1),\n",
              " (('task', 'lack', 'good'), 1),\n",
              " (('lack', 'good', 'weighting'), 1),\n",
              " (('good', 'weighting', 'schemes'), 1),\n",
              " (('weighting', 'schemes', 'compound'), 1),\n",
              " (('schemes', 'compound', 'index'), 1),\n",
              " (('compound', 'index', 'terms'), 1),\n",
              " (('index', 'terms', 'implicit'), 1),\n",
              " (('terms', 'implicit', 'linguistic'), 1),\n",
              " (('implicit', 'linguistic', 'processing'), 1),\n",
              " (('linguistic', 'processing', 'inherent'), 1),\n",
              " (('processing', 'inherent', 'statistical'), 1),\n",
              " (('inherent', 'statistical', 'methods'), 1),\n",
              " (('statistical', 'methods', 'natural'), 1),\n",
              " (('methods', 'natural', 'language'), 1),\n",
              " (('processing', 'techniques', 'may'), 1),\n",
              " (('techniques', 'may', 'important'), 1),\n",
              " (('may', 'important', 'work'), 1),\n",
              " (('important', 'work', 'computational'), 1),\n",
              " (('work', 'computational', 'linguistics'), 1),\n",
              " (('computational', 'linguistics', 'began'), 1),\n",
              " (('linguistics', 'began', 'soon'), 1),\n",
              " (('began', 'soon', 'development'), 1),\n",
              " (('soon', 'development', 'first'), 1),\n",
              " (('development', 'first', 'computers'), 1),\n",
              " (('first', 'computers', 'booth'), 1),\n",
              " (('computers', 'booth', 'brandwood'), 1),\n",
              " (('booth', 'brandwood', 'cleave'), 1),\n",
              " (('brandwood', 'cleave', 'yet'), 1),\n",
              " (('cleave', 'yet', 'intervening'), 1),\n",
              " (('yet', 'intervening', 'four'), 1),\n",
              " (('intervening', 'four', 'decades'), 1),\n",
              " (('four', 'decades', 'pervasive'), 1),\n",
              " (('decades', 'pervasive', 'feeling'), 1),\n",
              " (('pervasive', 'feeling', 'progress'), 1),\n",
              " (('feeling', 'progress', 'computer'), 1),\n",
              " (('progress', 'computer', 'understanding'), 1),\n",
              " (('computer', 'understanding', 'natural'), 1),\n",
              " (('understanding', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'commensurate'), 1),\n",
              " (('language', 'commensurate', 'voice'), 1),\n",
              " (('commensurate', 'voice', 'recognition'), 1),\n",
              " (('voice', 'recognition', 'natural'), 1),\n",
              " (('recognition', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'tamil'), 1),\n",
              " (('language', 'tamil', 'combining'), 1),\n",
              " (('tamil', 'combining', 'digital'), 1),\n",
              " (('combining', 'digital', 'mathematical'), 1),\n",
              " (('digital', 'mathematical', 'knowledge'), 1),\n",
              " (('mathematical', 'knowledge', 'using'), 1),\n",
              " (('knowledge', 'using', 'mfcc'), 1),\n",
              " (('using', 'mfcc', 'dtw'), 1),\n",
              " (('mfcc', 'dtw', 'extract'), 1),\n",
              " (('dtw', 'extract', 'match'), 1),\n",
              " (('extract', 'match', 'features'), 1),\n",
              " (('match', 'features', 'improve'), 1),\n",
              " (('features', 'improve', 'accuracy'), 1),\n",
              " (('improve', 'accuracy', 'better'), 1),\n",
              " (('accuracy', 'better', 'performance'), 1),\n",
              " (('better', 'performance', 'abstract'), 1),\n",
              " (('performance', 'abstract', 'testing'), 1),\n",
              " (('abstract', 'testing', 'natural'), 1),\n",
              " (('testing', 'natural', 'language'), 1),\n",
              " (('natural', 'language', 'requirements'), 1),\n",
              " (('language', 'requirements', 'standard'), 1),\n",
              " (('requirements', 'standard', 'approach'), 1),\n",
              " (('standard', 'approach', 'system'), 1),\n",
              " (('approach', 'system', 'acceptance'), 1),\n",
              " (('system', 'acceptance', 'testing'), 1),\n",
              " (('acceptance', 'testing', 'this'), 1),\n",
              " (('testing', 'this', 'test'), 1),\n",
              " (('this', 'test', 'often'), 1),\n",
              " (('test', 'often', 'performed'), 1),\n",
              " (('often', 'performed', 'independent'), 1),\n",
              " (('performed', 'independent', 'test'), 1),\n",
              " (('independent', 'test', 'organization'), 1),\n",
              " (('test', 'organization', 'unfamiliar'), 1),\n",
              " (('organization', 'unfamiliar', 'application'), 1),\n",
              " (('unfamiliar', 'application', 'area'), 1),\n",
              " (('application', 'area', 'the'), 1),\n",
              " (('area', 'the', 'things'), 1),\n",
              " (('the', 'things', 'testers'), 1),\n",
              " (('things', 'testers', 'go'), 1),\n",
              " (('testers', 'go', 'written'), 1),\n",
              " (('go', 'written', 'requirements'), 1),\n",
              " (('written', 'requirements', 'so'), 1),\n",
              " (('requirements', 'so', 'abstract'), 1),\n",
              " (('so', 'abstract', 'found'), 1),\n",
              " (('abstract', 'found', 'conversational'), 1),\n",
              " (('found', 'conversational', 'partners'), 1),\n",
              " (('conversational', 'partners', 'but'), 1),\n",
              " (('partners', 'but', 'also'), 1),\n",
              " (('but', 'also', 'provides'), 1),\n",
              " (('also', 'provides', 'us'), 1),\n",
              " (('provides', 'us', 'information'), 1),\n",
              " (('us', 'information', 'creative'), 1),\n",
              " (('information', 'creative', 'making'), 1),\n",
              " (('creative', 'making', 'associations'), 1),\n",
              " (('making', 'associations', 'storytelling'), 1),\n",
              " (('associations', 'storytelling', 'language'), 1),\n",
              " (('storytelling', 'language', 'use'), 1),\n",
              " (('language', 'use', 'many'), 1),\n",
              " (('use', 'many', 'subtleties'), 1),\n",
              " (('many', 'subtleties', 'face'), 1),\n",
              " (('subtleties', 'face', 'face'), 1),\n",
              " (('face', 'face', 'multiparty'), 1),\n",
              " (('face', 'multiparty', 'interaction'), 1),\n",
              " (('multiparty', 'interaction', 'added'), 1),\n",
              " (('interaction', 'added', 'using'), 1),\n",
              " (('added', 'using', 'humor'), 1),\n",
              " (('using', 'humor', 'persuade'), 1),\n",
              " (('humor', 'persuade', 'dominate'), 1),\n",
              " (('persuade', 'dominate', 'soften'), 1),\n",
              " (('dominate', 'soften', 'avoid'), 1),\n",
              " (('soften', 'avoid', 'face'), 1),\n",
              " (('avoid', 'face', 'threatening'), 1),\n",
              " (('face', 'threatening', 'act'), 1),\n",
              " (('threatening', 'act', 'abstract'), 1),\n",
              " (('act', 'abstract', 'found'), 1),\n",
              " (('found', 'in', 'recent'), 1),\n",
              " (('in', 'recent', 'years'), 1),\n",
              " (('recent', 'years', 'machine'), 1),\n",
              " (('years', 'machine', 'learning'), 1),\n",
              " (('learning', 'ml', 'used'), 1),\n",
              " (('ml', 'used', 'solve'), 1),\n",
              " (('used', 'solve', 'complex'), 1),\n",
              " (('solve', 'complex', 'tasks'), 1),\n",
              " (('complex', 'tasks', 'different'), 1),\n",
              " (('tasks', 'different', 'disciplines'), 1),\n",
              " (('different', 'disciplines', 'ranging'), 1),\n",
              " (('disciplines', 'ranging', 'data'), 1),\n",
              " (('ranging', 'data', 'mining'), 1),\n",
              " (('data', 'mining', 'information'), 1),\n",
              " (('mining', 'information', 'we'), 1),\n",
              " (('information', 'we', 'argue'), 1),\n",
              " (('we', 'argue', 'manual'), 1),\n",
              " (('argue', 'manual', 'automatic'), 1),\n",
              " (('manual', 'automatic', 'thesauruses'), 1),\n",
              " (('automatic', 'thesauruses', 'alternative'), 1),\n",
              " (('thesauruses', 'alternative', 'resources'), 1),\n",
              " (('alternative', 'resources', 'nlp'), 1),\n",
              " (('resources', 'nlp', 'tasks'), 1),\n",
              " (('nlp', 'tasks', 'this'), 1),\n",
              " (('tasks', 'this', 'involves'), 1),\n",
              " (('this', 'involves', 'radical'), 1),\n",
              " (('involves', 'radical', 'step'), 1),\n",
              " (('radical', 'step', 'interpreting'), 1),\n",
              " (('step', 'interpreting', 'manual'), 1),\n",
              " (('interpreting', 'manual', 'thesauruses'), 1),\n",
              " (('manual', 'thesauruses', 'classifications'), 1),\n",
              " (('thesauruses', 'classifications', 'words'), 1),\n",
              " (('classifications', 'words', 'rather'), 1),\n",
              " (('words', 'rather', 'word'), 1),\n",
              " (('rather', 'word', 'senses'), 1),\n",
              " (('word', 'senses', 'case'), 1),\n",
              " (('senses', 'case', 'made'), 1),\n",
              " (('case', 'made', 'the'), 1),\n",
              " (('made', 'the', 'range'), 1),\n",
              " (('the', 'range', 'roles'), 1),\n",
              " (('range', 'roles', 'thesauruses'), 1),\n",
              " (('roles', 'thesauruses', 'within'), 1),\n",
              " (('thesauruses', 'within', 'nlp'), 1),\n",
              " (('within', 'nlp', 'briefly'), 1),\n",
              " (('nlp', 'briefly', 'presented'), 1),\n",
              " (('briefly', 'presented', 'wasps'), 1),\n",
              " (('presented', 'wasps', 'thesaurus'), 1),\n",
              " (('wasps', 'thesaurus', 'introduced'), 1),\n",
              " (('thesaurus', 'introduced', 'thesaurus'), 1),\n",
              " (('introduced', 'thesaurus', 'evaluation'), 1),\n",
              " (('thesaurus', 'evaluation', 'becoming'), 1),\n",
              " (('evaluation', 'becoming', 'urgent'), 1),\n",
              " (('becoming', 'urgent', 'a'), 1),\n",
              " (('urgent', 'a', 'range'), 1),\n",
              " (('a', 'range', 'evaluation'), 1),\n",
              " (('range', 'evaluation', 'strategies'), 1),\n",
              " (('evaluation', 'strategies', 'embedded'), 1),\n",
              " (('strategies', 'embedded', 'within'), 1),\n",
              " (('embedded', 'within', 'nlp'), 1),\n",
              " (('within', 'nlp', 'tasks'), 1),\n",
              " (('nlp', 'tasks', 'proposed'), 1),\n",
              " (('tasks', 'proposed', 'introduction'), 1),\n",
              " (('proposed', 'introduction', 'patterns'), 1),\n",
              " (('introduction', 'patterns', 'music'), 1),\n",
              " (('patterns', 'music', 'object'), 1),\n",
              " (('music', 'object', 'intensive'), 1),\n",
              " (('object', 'intensive', 'studies'), 1),\n",
              " (('intensive', 'studies', 'past'), 1),\n",
              " (('studies', 'past', 'years'), 1),\n",
              " (('past', 'years', 'one'), 1),\n",
              " (('years', 'one', 'purposes'), 1),\n",
              " (('one', 'purposes', 'analyzing'), 1),\n",
              " (('purposes', 'analyzing', 'musical'), 1),\n",
              " (('analyzing', 'musical', 'structure'), 1),\n",
              " (('musical', 'structure', 'form'), 1),\n",
              " (('structure', 'form', 'discover'), 1),\n",
              " (('form', 'discover', 'patterns'), 1),\n",
              " (('discover', 'patterns', 'explicit'), 1),\n",
              " (('patterns', 'explicit', 'implicit'), 1),\n",
              " (('explicit', 'implicit', 'musical'), 1),\n",
              " (('implicit', 'musical', 'works'), 1),\n",
              " (('musical', 'works', 'simon'), 1),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_7OfwzJHI7s",
        "outputId": "79911510-82a2-4f08-f465-6272e7a0d595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1.2\n",
        "# Bigrams\n",
        "fd = nltk.FreqDist(nltk.bigrams(looping))\n",
        "fd_dict = dict(fd)\n",
        "for bi_gram in fd:\n",
        "  print(str(bi_gram) + '-' + str(fd_dict[bi_gram] / looping.count(bi_gram[1])))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('abstract', 'found')-1.0\n",
            "('found', 'describe')-0.1111111111111111\n",
            "('describe', 'method')-0.2\n",
            "('method', 'statistical')-0.125\n",
            "('statistical', 'modeling')-0.3333333333333333\n",
            "('modeling', 'based')-0.07692307692307693\n",
            "('based', 'maximum')-0.3333333333333333\n",
            "('maximum', 'entropy')-1.0\n",
            "('entropy', 'we')-0.041666666666666664\n",
            "('we', 'present')-0.5\n",
            "('present', 'maximum')-0.3333333333333333\n",
            "('maximum', 'likelihood')-0.5\n",
            "('likelihood', 'approach')-0.09090909090909091\n",
            "('approach', 'automatically')-0.3333333333333333\n",
            "('automatically', 'constructing')-1.0\n",
            "('constructing', 'maximum')-0.3333333333333333\n",
            "('entropy', 'models')-0.25\n",
            "('models', 'describe')-0.1111111111111111\n",
            "('describe', 'implement')-1.0\n",
            "('implement', 'approach')-0.09090909090909091\n",
            "('approach', 'efficiently')-1.0\n",
            "('efficiently', 'using')-0.09090909090909091\n",
            "('using', 'examples')-1.0\n",
            "('examples', 'several')-0.2\n",
            "('several', 'problems')-0.3333333333333333\n",
            "('problems', 'natural')-0.010416666666666666\n",
            "('natural', 'language')-0.8165137614678899\n",
            "('language', 'processing')-0.9294117647058824\n",
            "('processing', 'scaling')-1.0\n",
            "('scaling', 'conditional')-1.0\n",
            "('conditional', 'random')-1.0\n",
            "('random', 'fields')-0.3333333333333333\n",
            "('fields', 'natural')-0.010416666666666666\n",
            "('processing', 'terms')-0.3333333333333333\n",
            "('terms', 'conditions')-1.0\n",
            "('conditions', 'terms')-0.3333333333333333\n",
            "('conditions', 'copyright')-1.0\n",
            "('copyright', 'works')-0.5\n",
            "('works', 'deposited')-1.0\n",
            "('deposited', 'minerva')-1.0\n",
            "('minerva', 'access')-1.0\n",
            "('access', 'retained')-1.0\n",
            "('retained', 'the')-0.038461538461538464\n",
            "('the', 'paper')-0.09523809523809523\n",
            "('paper', 'addresses')-0.5\n",
            "('addresses', 'issue')-0.3333333333333333\n",
            "('issue', 'cooperation')-0.5\n",
            "('cooperation', 'linguistics')-0.2\n",
            "('linguistics', 'natural')-0.010416666666666666\n",
            "('processing', 'nlp')-0.4666666666666667\n",
            "('nlp', 'general')-0.3333333333333333\n",
            "('general', 'linguistics')-0.2\n",
            "('linguistics', 'machine')-0.058823529411764705\n",
            "('machine', 'translation')-1.0\n",
            "('translation', 'mt')-1.0\n",
            "('mt', 'particular')-0.2\n",
            "('particular', 'it')-0.2\n",
            "('it', 'focuses')-0.3333333333333333\n",
            "('focuses', 'one')-0.125\n",
            "('one', 'direction')-1.0\n",
            "('direction', 'cooperation')-0.5\n",
            "('cooperation', 'namely')-1.0\n",
            "('namely', 'applications')-0.08333333333333333\n",
            "('applications', 'linguistics')-0.2\n",
            "('linguistics', 'nlp')-0.022222222222222223\n",
            "('nlp', 'virtually')-1.0\n",
            "('virtually', 'in')-0.058823529411764705\n",
            "('in', 'natural')-0.020833333333333332\n",
            "('processing', 'applications')-0.16666666666666666\n",
            "('applications', 'description')-0.25\n",
            "('description', 'logics')-1.0\n",
            "('logics', 'used')-0.09090909090909091\n",
            "('used', 'encode')-0.5\n",
            "('encode', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'base')-1.0\n",
            "('base', 'syntactic')-0.16666666666666666\n",
            "('syntactic', 'semantic')-0.25\n",
            "('semantic', 'pragmatic')-1.0\n",
            "('pragmatic', 'elements')-0.3333333333333333\n",
            "('elements', 'needed')-1.0\n",
            "('needed', 'drive')-0.5\n",
            "('drive', 'semantic')-0.125\n",
            "('semantic', 'interpretation')-0.6666666666666666\n",
            "('interpretation', 'natural')-0.010416666666666666\n",
            "('language', 'generation')-0.5\n",
            "('generation', 'processes')-0.2\n",
            "('processes', 'more')-1.0\n",
            "('more', 'recently')-0.5\n",
            "('recently', 'description')-0.25\n",
            "('logics', 'we')-0.041666666666666664\n",
            "('we', 'propose')-1.0\n",
            "('propose', 'unified')-1.0\n",
            "('unified', 'neural')-0.3333333333333333\n",
            "('neural', 'network')-0.5\n",
            "('network', 'architecture')-0.5\n",
            "('architecture', 'learning')-0.047619047619047616\n",
            "('learning', 'algorithm')-1.0\n",
            "('algorithm', 'applied')-0.2\n",
            "('applied', 'various')-0.125\n",
            "('various', 'natural')-0.010416666666666666\n",
            "('processing', 'tasks')-0.125\n",
            "('tasks', 'including')-0.125\n",
            "('including', 'part')-0.16666666666666666\n",
            "('part', 'speech')-0.2727272727272727\n",
            "('speech', 'tagging')-1.0\n",
            "('tagging', 'chunking')-0.5\n",
            "('chunking', 'named')-0.5\n",
            "('named', 'entity')-1.0\n",
            "('entity', 'recognition')-0.08333333333333333\n",
            "('recognition', 'semantic')-0.125\n",
            "('semantic', 'role')-0.3333333333333333\n",
            "('role', 'labeling')-1.0\n",
            "('labeling', 'this')-0.034482758620689655\n",
            "('this', 'versatility')-1.0\n",
            "('versatility', 'achieved')-1.0\n",
            "('achieved', 'trying')-1.0\n",
            "('trying', 'avoid')-0.3333333333333333\n",
            "('avoid', 'task')-0.2\n",
            "('task', 'natural')-0.010416666666666666\n",
            "('processing', 'the')-0.11538461538461539\n",
            "('the', 'subject')-0.5\n",
            "('subject', 'natural')-0.010416666666666666\n",
            "('processing', 'considered')-1.0\n",
            "('considered', 'broad')-0.3333333333333333\n",
            "('broad', 'narrow')-1.0\n",
            "('narrow', 'senses')-0.5\n",
            "('senses', 'in')-0.058823529411764705\n",
            "('in', 'broad')-0.3333333333333333\n",
            "('broad', 'sense')-0.08333333333333333\n",
            "('sense', 'covers')-1.0\n",
            "('covers', 'processing')-0.011764705882352941\n",
            "('processing', 'issues')-0.5\n",
            "('issues', 'levels')-0.5\n",
            "('levels', 'natural')-0.010416666666666666\n",
            "('language', 'understanding')-0.3333333333333333\n",
            "('understanding', 'including')-0.125\n",
            "('including', 'speech')-0.09090909090909091\n",
            "('speech', 'recognition')-0.3333333333333333\n",
            "('recognition', 'syntactic')-0.16666666666666666\n",
            "('semantic', 'analysis')-0.09090909090909091\n",
            "('analysis', 'sentences')-1.0\n",
            "('sentences', 'robots')-0.5\n",
            "('robots', 'interact')-1.0\n",
            "('interact', 'humans')-0.25\n",
            "('humans', 'face')-0.2\n",
            "('face', 'face')-0.4\n",
            "('face', 'using')-0.09090909090909091\n",
            "('using', 'natural')-0.020833333333333332\n",
            "('language', 'need')-0.16666666666666666\n",
            "('need', 'responsive')-1.0\n",
            "('responsive', 'way')-0.2\n",
            "('way', 'humans')-0.5\n",
            "('humans', 'use')-0.08333333333333333\n",
            "('use', 'language')-0.009174311926605505\n",
            "('language', 'situations')-1.0\n",
            "('situations', 'we')-0.041666666666666664\n",
            "('propose', 'psychologicallyinspired')-1.0\n",
            "('psychologicallyinspired', 'natural')-0.010416666666666666\n",
            "('processing', 'system')-0.07142857142857142\n",
            "('system', 'robots')-0.5\n",
            "('robots', 'performs')-0.5\n",
            "('performs', 'incremental')-1.0\n",
            "('incremental', 'semantic')-0.125\n",
            "('interpretation', 'spoken')-0.25\n",
            "('spoken', 'utterances')-1.0\n",
            "('utterances', 'natural')-0.010416666666666666\n",
            "('natural', 'languages')-0.5\n",
            "('languages', 'languages')-0.3333333333333333\n",
            "('languages', 'spoken')-0.5\n",
            "('spoken', 'humans')-0.25\n",
            "('humans', 'currently')-0.5\n",
            "('currently', 'yet')-0.5\n",
            "('yet', 'point')-0.25\n",
            "('point', 'languages')-0.16666666666666666\n",
            "('languages', 'unprocessed')-1.0\n",
            "('unprocessed', 'forms')-0.3333333333333333\n",
            "('forms', 'understood')-0.5\n",
            "('understood', 'computers')-0.3333333333333333\n",
            "('computers', 'natural')-0.010416666666666666\n",
            "('processing', 'collection')-0.5\n",
            "('collection', 'techniques')-0.07142857142857142\n",
            "('techniques', 'employed')-0.5\n",
            "('employed', 'try')-1.0\n",
            "('try', 'accomplish')-1.0\n",
            "('accomplish', 'goal')-0.3333333333333333\n",
            "('goal', 'the')-0.038461538461538464\n",
            "('the', 'field')-0.2\n",
            "('field', 'natural')-0.010416666666666666\n",
            "('natural', 'abstract')-0.05\n",
            "('abstract', 'ambiguity')-1.0\n",
            "('ambiguity', 'referred')-1.0\n",
            "('referred', 'ability')-1.0\n",
            "('ability', 'one')-0.125\n",
            "('one', 'meaning')-0.5\n",
            "('meaning', 'understood')-0.5\n",
            "('understood', 'one')-0.125\n",
            "('one', 'way')-0.2\n",
            "('way', 'natural')-0.010416666666666666\n",
            "('languages', 'ambiguous')-1.0\n",
            "('ambiguous', 'computers')-0.3333333333333333\n",
            "('computers', 'able')-0.5\n",
            "('able', 'understand')-1.0\n",
            "('understand', 'language')-0.009174311926605505\n",
            "('language', 'way')-0.4\n",
            "('way', 'people')-1.0\n",
            "('people', 'natural')-0.010416666666666666\n",
            "('nlp', 'concerned')-1.0\n",
            "('concerned', 'development')-0.1\n",
            "('development', 'introduction')-0.2\n",
            "('introduction', 'statistical')-0.125\n",
            "('statistical', 'natural')-0.020833333333333332\n",
            "('processing', 'snlp')-0.5\n",
            "('snlp', 'field')-0.2\n",
            "('field', 'lying')-1.0\n",
            "('lying', 'intersection')-1.0\n",
            "('intersection', 'natural')-0.010416666666666666\n",
            "('processing', 'machine')-0.058823529411764705\n",
            "('machine', 'learning')-0.47619047619047616\n",
            "('learning', 'snlp')-0.5\n",
            "('snlp', 'di')-1.0\n",
            "('di', 'ers')-1.0\n",
            "('ers', 'traditional')-0.3333333333333333\n",
            "('traditional', 'natural')-0.010416666666666666\n",
            "('processing', 'instead')-1.0\n",
            "('instead', 'linguist')-1.0\n",
            "('linguist', 'manually')-0.5\n",
            "('manually', 'construct')-1.0\n",
            "('construct', 'model')-0.14285714285714285\n",
            "('model', 'given')-0.2\n",
            "('given', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'text')-0.09090909090909091\n",
            "('text', 'directly')-1.0\n",
            "('directly', 'rather')-0.3333333333333333\n",
            "('rather', 'e')-1.0\n",
            "('e', 'g')-1.0\n",
            "('g', 'titles')-1.0\n",
            "('titles', 'abstracts')-1.0\n",
            "('abstracts', 'suggests')-1.0\n",
            "('suggests', 'appropriate')-1.0\n",
            "('appropriate', 'approaches')-0.16666666666666666\n",
            "('approaches', 'focus')-0.16666666666666666\n",
            "('focus', 'role')-0.3333333333333333\n",
            "('role', 'natural')-0.010416666666666666\n",
            "('paper', 'also')-0.2\n",
            "('also', 'comments')-1.0\n",
            "('comments', 'possible')-0.5\n",
            "('possible', 'connections')-1.0\n",
            "('connections', 'data')-0.125\n",
            "('data', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'retrieval')-0.1\n",
            "('retrieval', 'concludes')-1.0\n",
            "('concludes', 'emphasizing')-1.0\n",
            "('emphasizing', 'importance')-1.0\n",
            "('importance', 'rigorous')-1.0\n",
            "('rigorous', 'abstract')-0.05\n",
            "('abstract', 'language')-0.009174311926605505\n",
            "('way', 'communicating')-1.0\n",
            "('communicating', 'words')-0.16666666666666666\n",
            "('words', 'language')-0.009174311926605505\n",
            "('language', 'helps')-1.0\n",
            "('helps', 'understanding')-0.16666666666666666\n",
            "('understanding', 'world')-0.5\n",
            "('world', 'get')-1.0\n",
            "('get', 'better')-0.5\n",
            "('better', 'insight')-1.0\n",
            "('insight', 'world')-0.5\n",
            "('world', 'language')-0.009174311926605505\n",
            "('helps', 'speakers')-1.0\n",
            "('speakers', 'vague')-1.0\n",
            "('vague', 'precise')-1.0\n",
            "('precise', 'like')-0.3333333333333333\n",
            "('like', 'nlp')-0.022222222222222223\n",
            "('nlp', 'stands')-1.0\n",
            "('stands', 'natural')-0.010416666666666666\n",
            "('processing', 'natural')-0.010416666666666666\n",
            "('spoken', 'we')-0.041666666666666664\n",
            "('we', 'report')-0.5\n",
            "('report', 'experiments')-0.5\n",
            "('experiments', 'use')-0.08333333333333333\n",
            "('use', 'standard')-0.3333333333333333\n",
            "('standard', 'natural')-0.010416666666666666\n",
            "('nlp', 'tools')-0.3333333333333333\n",
            "('tools', 'analysis')-0.09090909090909091\n",
            "('analysis', 'music')-0.25\n",
            "('music', 'lyrics')-0.3333333333333333\n",
            "('lyrics', 'a')-0.25\n",
            "('a', 'significant')-0.3333333333333333\n",
            "('significant', 'amount')-1.0\n",
            "('amount', 'music')-0.25\n",
            "('music', 'audio')-1.0\n",
            "('audio', 'lyrics')-0.3333333333333333\n",
            "('lyrics', 'lyrics')-0.3333333333333333\n",
            "('lyrics', 'encode')-0.5\n",
            "('encode', 'important')-0.3333333333333333\n",
            "('important', 'part')-0.16666666666666666\n",
            "('part', 'semantics')-0.5\n",
            "('semantics', 'song')-1.0\n",
            "('song', 'therefore')-1.0\n",
            "('therefore', 'analysis')-0.09090909090909091\n",
            "('analysis', 'complements')-1.0\n",
            "('complements', 'acoustic')-1.0\n",
            "('acoustic', 'cultural')-1.0\n",
            "('cultural', 'paper')-0.047619047619047616\n",
            "('paper', 'describe')-0.3333333333333333\n",
            "('describe', 'simple')-0.5\n",
            "('simple', 'rule')-1.0\n",
            "('rule', 'based')-0.07692307692307693\n",
            "('based', 'approach')-0.09090909090909091\n",
            "('approach', 'automated')-0.2\n",
            "('automated', 'learning')-0.047619047619047616\n",
            "('learning', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'knowledge')-0.16666666666666666\n",
            "('knowledge', 'this')-0.034482758620689655\n",
            "('this', 'approach')-0.09090909090909091\n",
            "('approach', 'shown')-0.5\n",
            "('shown', 'number')-0.2\n",
            "('number', 'tasks')-0.125\n",
            "('tasks', 'capture')-0.3333333333333333\n",
            "('capture', 'information')-0.045454545454545456\n",
            "('information', 'clearer')-1.0\n",
            "('clearer', 'direct')-1.0\n",
            "('direct', 'fashion')-1.0\n",
            "('fashion', 'without')-1.0\n",
            "('without', 'compromise')-1.0\n",
            "('compromise', 'performance')-0.25\n",
            "('performance', 'we')-0.041666666666666664\n",
            "('present', 'detailed')-1.0\n",
            "('detailed', 'case')-0.5\n",
            "('case', 'study')-0.5\n",
            "('study', 'learning')-0.047619047619047616\n",
            "('learning', 'method')-0.2\n",
            "('method', 'applied')-0.2\n",
            "('applied', 'part')-0.16666666666666666\n",
            "('tagging', 'this')-0.034482758620689655\n",
            "('this', 'paper')-0.47619047619047616\n",
            "('paper', 'focuses')-0.3333333333333333\n",
            "('focuses', 'connectionist')-1.0\n",
            "('connectionist', 'models')-0.25\n",
            "('models', 'natural')-0.010416666666666666\n",
            "('processing', 'we')-0.125\n",
            "('we', 'briefly')-0.3333333333333333\n",
            "('briefly', 'present')-0.25\n",
            "('present', 'discuss')-0.5\n",
            "('discuss', 'several')-0.4\n",
            "('several', 'aspects')-0.5\n",
            "('aspects', 'high')-0.3333333333333333\n",
            "('high', 'level')-0.6666666666666666\n",
            "('level', 'tasks')-0.0625\n",
            "('tasks', 'recently')-0.5\n",
            "('recently', 'approached')-1.0\n",
            "('approached', 'connectionism')-1.0\n",
            "('connectionism', 'either')-1.0\n",
            "('either', 'localist')-1.0\n",
            "('localist', 'parallel')-0.5\n",
            "('parallel', 'distributed')-1.0\n",
            "('distributed', 'processing')-0.011764705882352941\n",
            "('processing', 'models')-0.25\n",
            "('models', 'several')-0.2\n",
            "('several', 'interesting')-1.0\n",
            "('interesting', 'architectures')-1.0\n",
            "('architectures', 'process')-0.125\n",
            "('process', 'language')-0.009174311926605505\n",
            "('understanding', 'this')-0.034482758620689655\n",
            "('this', 'new')-0.16666666666666666\n",
            "('new', 'approach')-0.09090909090909091\n",
            "('approach', 'natural')-0.010416666666666666\n",
            "('processing', 'based')-0.07692307692307693\n",
            "('based', 'deterministic')-1.0\n",
            "('deterministic', 'chaotic')-1.0\n",
            "('chaotic', 'behavior')-1.0\n",
            "('behavior', 'dynamical')-1.0\n",
            "('dynamical', 'systems')-0.0625\n",
            "('systems', 'paper')-0.047619047619047616\n",
            "('paper', 'see')-1.0\n",
            "('see', 'schank')-1.0\n",
            "('schank', 'theoretical')-1.0\n",
            "('theoretical', 'discussion')-1.0\n",
            "('discussion', 'kass')-1.0\n",
            "('kass', 'leake')-1.0\n",
            "('leake', 'owens')-1.0\n",
            "('owens', 'brief')-0.3333333333333333\n",
            "('brief', 'discussions')-1.0\n",
            "('discussions', 'program')-0.3333333333333333\n",
            "('program', 'built')-0.5\n",
            "('built', 'around')-1.0\n",
            "('around', 'principles')-0.3333333333333333\n",
            "('principles', 'goal')-0.3333333333333333\n",
            "('goal', 'simply')-1.0\n",
            "('simply', 'point')-0.25\n",
            "('point', 'interest')-1.0\n",
            "('interest', 'natural')-0.010416666666666666\n",
            "('processing', 'led')-1.0\n",
            "('led', 'us')-0.5\n",
            "('us', 'naturally')-0.5\n",
            "('naturally', 'indeed')-1.0\n",
            "('indeed', 'inevitably')-1.0\n",
            "('inevitably', 'objectives')-0.5\n",
            "('objectives', 'to')-1.0\n",
            "('to', 'provide')-1.0\n",
            "('provide', 'overview')-0.5\n",
            "('overview', 'tutorial')-0.5\n",
            "('tutorial', 'natural')-0.010416666666666666\n",
            "('nlp', 'modern')-1.0\n",
            "('modern', 'nlp')-0.022222222222222223\n",
            "('nlp', 'system')-0.14285714285714285\n",
            "('system', 'design')-0.25\n",
            "('design', 'target')-1.0\n",
            "('target', 'audience')-1.0\n",
            "('audience', 'this')-0.034482758620689655\n",
            "('this', 'tutorial')-0.5\n",
            "('tutorial', 'targets')-1.0\n",
            "('targets', 'medical')-1.0\n",
            "('medical', 'informatics')-1.0\n",
            "('informatics', 'generalist')-1.0\n",
            "('generalist', 'limited')-0.25\n",
            "('limited', 'acquaintance')-1.0\n",
            "('acquaintance', 'principles')-0.3333333333333333\n",
            "('principles', 'behind')-1.0\n",
            "('behind', 'nlp')-0.022222222222222223\n",
            "('nlp', 'limited')-0.25\n",
            "('limited', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'current')-0.14285714285714285\n",
            "('current', 'state')-0.25\n",
            "('state', 'this')-0.034482758620689655\n",
            "('paper', 'briefly')-0.3333333333333333\n",
            "('briefly', 'describes')-0.5\n",
            "('describes', 'current')-0.14285714285714285\n",
            "('current', 'implementation')-0.5\n",
            "('implementation', 'status')-1.0\n",
            "('status', 'intelligent')-0.5\n",
            "('intelligent', 'information')-0.045454545454545456\n",
            "('information', 'retrieval')-0.6\n",
            "('retrieval', 'system')-0.14285714285714285\n",
            "('system', 'marie')-1.0\n",
            "('marie', 'employs')-1.0\n",
            "('employs', 'natural')-0.010416666666666666\n",
            "('processing', 'techniques')-0.21428571428571427\n",
            "('techniques', 'descriptive')-1.0\n",
            "('descriptive', 'captions')-0.5\n",
            "('captions', 'used')-0.09090909090909091\n",
            "('used', 'iden')-1.0\n",
            "('iden', 'tify')-1.0\n",
            "('tify', 'photographic')-1.0\n",
            "('photographic', 'images')-0.5\n",
            "('images', 'concerning')-0.5\n",
            "('concerning', 'various')-0.125\n",
            "('various', 'military')-1.0\n",
            "('military', 'projects')-0.5\n",
            "('projects', 'the')-0.038461538461538464\n",
            "('the', 'captions')-0.5\n",
            "('captions', 'parsed')-1.0\n",
            "('parsed', 'based')-0.07692307692307693\n",
            "('based', 'literature')-1.0\n",
            "('literature', 'resources')-0.2\n",
            "('resources', 'we')-0.041666666666666664\n",
            "('we', 'describe')-0.4444444444444444\n",
            "('describe', 'system')-0.07142857142857142\n",
            "('system', 'agent')-1.0\n",
            "('agent', 'directed')-1.0\n",
            "('directed', 'natural')-0.010416666666666666\n",
            "('processing', 'extract')-0.25\n",
            "('extract', 'information')-0.045454545454545456\n",
            "('information', 'journal')-1.0\n",
            "('journal', 'articles')-1.0\n",
            "('articles', 'an')-0.5\n",
            "('an', 'interface')-0.5\n",
            "('interface', 'developed')-0.2\n",
            "('developed', 'permit')-1.0\n",
            "('permit', 'curation')-1.0\n",
            "('curation', 'nlp')-0.022222222222222223\n",
            "('nlp', 'results')-0.14285714285714285\n",
            "('results', 'deposition')-1.0\n",
            "('deposition', 'accepted')-1.0\n",
            "('accepted', 'results')-0.14285714285714285\n",
            "('results', 'knowledge')-0.08333333333333333\n",
            "('base', 'motivation')-0.5\n",
            "('motivation', 'the')-0.038461538461538464\n",
            "('the', 'advent')-1.0\n",
            "('advent', 'high')-0.3333333333333333\n",
            "('high', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'speech')-0.09090909090909091\n",
            "('speech', 'processing')-0.011764705882352941\n",
            "('processing', 'part')-0.16666666666666666\n",
            "('part', 'surveys')-1.0\n",
            "('surveys', 'significant')-0.3333333333333333\n",
            "('significant', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'work')-0.125\n",
            "('work', 'done')-1.0\n",
            "('done', 'far')-0.5\n",
            "('far', 'instance')-1.0\n",
            "('instance', 'machine')-0.058823529411764705\n",
            "('translation', 'discusses')-1.0\n",
            "('discusses', 'particular')-0.2\n",
            "('particular', 'problems')-0.3333333333333333\n",
            "('problems', 'generic')-0.3333333333333333\n",
            "('generic', 'system')-0.07142857142857142\n",
            "('system', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'the')-0.038461538461538464\n",
            "('the', 'conclusion')-1.0\n",
            "('conclusion', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'strategies')-1.0\n",
            "('strategies', 'techniques')-0.07142857142857142\n",
            "('techniques', 'nlp')-0.022222222222222223\n",
            "('nlp', 'need')-0.16666666666666666\n",
            "('need', 'much')-0.5\n",
            "('much', 'development')-0.1\n",
            "('development', 'particular')-0.2\n",
            "('particular', 'similar')-0.5\n",
            "('similar', 'way')-0.2\n",
            "('humans', 'intuitively')-0.5\n",
            "('intuitively', 'order')-0.5\n",
            "('order', 'eliminate')-1.0\n",
            "('eliminate', 'noisy')-1.0\n",
            "('noisy', 'content')-1.0\n",
            "('content', 'in')-0.058823529411764705\n",
            "('in', 'paper')-0.23809523809523808\n",
            "('describe', 'combination')-1.0\n",
            "('combination', 'html')-1.0\n",
            "('html', 'dom')-1.0\n",
            "('dom', 'analysis')-0.09090909090909091\n",
            "('analysis', 'natural')-0.010416666666666666\n",
            "('nlp', 'techniques')-0.14285714285714285\n",
            "('techniques', 'automated')-0.2\n",
            "('automated', 'extractions')-1.0\n",
            "('extractions', 'main')-1.0\n",
            "('main', 'article')-0.3333333333333333\n",
            "('article', 'associated')-1.0\n",
            "('associated', 'images')-0.5\n",
            "('images', 'web')-0.2\n",
            "('web', 'pages')-1.0\n",
            "('pages', 'abstract')-0.05\n",
            "('abstract', 'natural')-0.041666666666666664\n",
            "('processing', 'theoretically')-1.0\n",
            "('theoretically', 'motivated')-1.0\n",
            "('motivated', 'range')-0.16666666666666666\n",
            "('range', 'computational')-0.2\n",
            "('computational', 'techniques')-0.07142857142857142\n",
            "('techniques', 'analysing')-1.0\n",
            "('analysing', 'representing')-1.0\n",
            "('representing', 'naturally')-0.5\n",
            "('naturally', 'occurring')-1.0\n",
            "('occurring', 'texts')-0.5\n",
            "('texts', 'one')-0.125\n",
            "('one', 'levels')-0.5\n",
            "('levels', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'analysis')-0.09090909090909091\n",
            "('analysis', 'purpose')-0.5\n",
            "('purpose', 'achieving')-1.0\n",
            "('achieving', 'human')-0.5\n",
            "('human', 'like')-0.3333333333333333\n",
            "('like', 'language')-0.009174311926605505\n",
            "('processing', 'range')-0.16666666666666666\n",
            "('range', 'tasks')-0.0625\n",
            "('tasks', 'this')-0.06896551724137931\n",
            "('paper', 'reviews')-1.0\n",
            "('reviews', 'processes')-0.2\n",
            "('processes', 'involved')-1.0\n",
            "('involved', 'natural')-0.010416666666666666\n",
            "('nlp', 'it')-0.2\n",
            "('it', 'demonstrates')-1.0\n",
            "('demonstrates', 'various')-0.125\n",
            "('various', 'kinds')-0.5\n",
            "('kinds', 'choices')-1.0\n",
            "('choices', 'need')-0.16666666666666666\n",
            "('need', 'taken')-0.5\n",
            "('taken', 'execution')-0.5\n",
            "('execution', 'word')-0.1111111111111111\n",
            "('word', 'morphology')-1.0\n",
            "('morphology', 'syntactic')-0.16666666666666666\n",
            "('syntactic', 'text')-0.09090909090909091\n",
            "('text', 'analysis')-0.18181818181818182\n",
            "('analysis', 'text')-0.18181818181818182\n",
            "('text', 'generation')-0.5\n",
            "('generation', 'components')-0.5\n",
            "('components', 'it')-0.2\n",
            "('it', 'compares')-1.0\n",
            "('compares', 'time')-0.5\n",
            "('time', 'complexity')-0.25\n",
            "('complexity', 'this')-0.034482758620689655\n",
            "('this', 'article')-0.6666666666666666\n",
            "('article', 'focusses')-1.0\n",
            "('focusses', 'derivation')-1.0\n",
            "('derivation', 'large')-0.5\n",
            "('large', 'lexicons')-1.0\n",
            "('lexicons', 'natural')-0.010416666666666666\n",
            "('describe', 'development')-0.1\n",
            "('development', 'dictionary')-0.3333333333333333\n",
            "('dictionary', 'support')-0.3333333333333333\n",
            "('support', 'environment')-0.5\n",
            "('environment', 'linking')-1.0\n",
            "('linking', 'restructured')-1.0\n",
            "('restructured', 'version')-0.5\n",
            "('version', 'longman')-1.0\n",
            "('longman', 'dictionary')-0.3333333333333333\n",
            "('dictionary', 'contemporary')-1.0\n",
            "('contemporary', 'english')-0.5\n",
            "('english', 'natural')-0.010416666666666666\n",
            "('processing', 'systems')-0.3125\n",
            "('systems', 'the')-0.11538461538461539\n",
            "('the', 'process')-0.125\n",
            "('process', 'we')-0.041666666666666664\n",
            "('we', 'introduce')-1.0\n",
            "('introduce', 'method')-0.2\n",
            "('method', 'analyzing')-0.5\n",
            "('analyzing', 'complexity')-0.25\n",
            "('complexity', 'natural')-0.010416666666666666\n",
            "('tasks', 'predicting')-1.0\n",
            "('predicting', 'difficulty')-1.0\n",
            "('difficulty', 'new')-0.16666666666666666\n",
            "('new', 'nlp')-0.022222222222222223\n",
            "('nlp', 'tasks')-0.375\n",
            "('tasks', 'our')-0.5\n",
            "('our', 'complexity')-0.25\n",
            "('complexity', 'measures')-0.3333333333333333\n",
            "('measures', 'derived')-1.0\n",
            "('derived', 'kolmogorov')-1.0\n",
            "('kolmogorov', 'complexity')-0.25\n",
            "('complexity', 'class')-0.3333333333333333\n",
            "('class', 'automata')-0.5\n",
            "('automata', 'meaning')-0.5\n",
            "('meaning', 'automata')-0.5\n",
            "('automata', 'whose')-1.0\n",
            "('whose', 'purpose')-0.5\n",
            "('purpose', 'extract')-0.25\n",
            "('extract', 'relevant')-0.3333333333333333\n",
            "('relevant', 'pieces')-1.0\n",
            "('pieces', 'sounds')-1.0\n",
            "('sounds', 'text')-0.09090909090909091\n",
            "('text', 'motion')-1.0\n",
            "('motion', 'the')-0.038461538461538464\n",
            "('the', 'techniques')-0.07142857142857142\n",
            "('techniques', 'developed')-0.2\n",
            "('developed', 'deep')-0.5\n",
            "('deep', 'learning')-0.09523809523809523\n",
            "('learning', 'research')-0.08333333333333333\n",
            "('research', 'already')-1.0\n",
            "('already', 'impacting')-1.0\n",
            "('impacting', 'research')-0.08333333333333333\n",
            "('research', 'natural')-0.010416666666666666\n",
            "('language', 'process')-0.125\n",
            "('process', 'this')-0.034482758620689655\n",
            "('reviews', 'recent')-0.16666666666666666\n",
            "('recent', 'research')-0.25\n",
            "('research', 'deep')-0.5\n",
            "('learning', 'applications')-0.08333333333333333\n",
            "('applications', 'recent')-0.16666666666666666\n",
            "('recent', 'development')-0.2\n",
            "('development', 'natural')-0.020833333333333332\n",
            "('processing', 'this')-0.034482758620689655\n",
            "('this', 'author')-1.0\n",
            "('author', 'produced')-0.5\n",
            "('produced', 'version')-0.5\n",
            "('version', 'paper')-0.047619047619047616\n",
            "('paper', 'published')-1.0\n",
            "('published', 'the')-0.038461538461538464\n",
            "('the', 'abstract')-0.05\n",
            "('nlp', 'application')-0.2\n",
            "('application', 'automated')-0.2\n",
            "('automated', 'parsing')-0.25\n",
            "('parsing', 'machine')-0.058823529411764705\n",
            "('learning', 'techniques')-0.14285714285714285\n",
            "('techniques', 'analyze')-1.0\n",
            "('analyze', 'standard')-0.3333333333333333\n",
            "('standard', 'text')-0.09090909090909091\n",
            "('text', 'applications')-0.08333333333333333\n",
            "('applications', 'nlp')-0.022222222222222223\n",
            "('nlp', 'requirements')-0.2\n",
            "('requirements', 'engineering')-0.5\n",
            "('engineering', 'include')-0.5\n",
            "('include', 'extraction')-0.3333333333333333\n",
            "('extraction', 'ontologies')-1.0\n",
            "('ontologies', 'requirements')-0.2\n",
            "('requirements', 'specification')-1.0\n",
            "('specification', 'use')-0.08333333333333333\n",
            "('use', 'nlp')-0.022222222222222223\n",
            "('nlp', 'verify')-1.0\n",
            "('verify', 'consistency')-1.0\n",
            "('consistency', 'statistical')-0.125\n",
            "('statistical', 'baseline')-1.0\n",
            "('baseline', 'including')-0.125\n",
            "('including', 'forgiving')-1.0\n",
            "('forgiving', 'nature')-1.0\n",
            "('nature', 'broad')-0.3333333333333333\n",
            "('broad', 'coverage')-1.0\n",
            "('coverage', 'typical')-1.0\n",
            "('typical', 'retrieval')-0.1\n",
            "('retrieval', 'task')-0.2\n",
            "('task', 'lack')-1.0\n",
            "('lack', 'good')-0.5\n",
            "('good', 'weighting')-0.5\n",
            "('weighting', 'schemes')-1.0\n",
            "('schemes', 'compound')-0.3333333333333333\n",
            "('compound', 'index')-1.0\n",
            "('index', 'terms')-0.3333333333333333\n",
            "('terms', 'implicit')-0.5\n",
            "('implicit', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'processing')-0.011764705882352941\n",
            "('processing', 'inherent')-1.0\n",
            "('inherent', 'statistical')-0.125\n",
            "('statistical', 'methods')-0.3333333333333333\n",
            "('methods', 'natural')-0.010416666666666666\n",
            "('techniques', 'may')-0.3333333333333333\n",
            "('may', 'important')-0.3333333333333333\n",
            "('important', 'work')-0.125\n",
            "('work', 'computational')-0.2\n",
            "('computational', 'linguistics')-0.4\n",
            "('linguistics', 'began')-1.0\n",
            "('began', 'soon')-1.0\n",
            "('soon', 'development')-0.1\n",
            "('development', 'first')-0.3333333333333333\n",
            "('first', 'computers')-0.3333333333333333\n",
            "('computers', 'booth')-1.0\n",
            "('booth', 'brandwood')-1.0\n",
            "('brandwood', 'cleave')-1.0\n",
            "('cleave', 'yet')-0.5\n",
            "('yet', 'intervening')-1.0\n",
            "('intervening', 'four')-1.0\n",
            "('four', 'decades')-1.0\n",
            "('decades', 'pervasive')-1.0\n",
            "('pervasive', 'feeling')-1.0\n",
            "('feeling', 'progress')-1.0\n",
            "('progress', 'computer')-1.0\n",
            "('computer', 'understanding')-0.16666666666666666\n",
            "('understanding', 'natural')-0.010416666666666666\n",
            "('language', 'commensurate')-1.0\n",
            "('commensurate', 'voice')-1.0\n",
            "('voice', 'recognition')-0.08333333333333333\n",
            "('recognition', 'natural')-0.010416666666666666\n",
            "('language', 'tamil')-1.0\n",
            "('tamil', 'combining')-1.0\n",
            "('combining', 'digital')-0.3333333333333333\n",
            "('digital', 'mathematical')-0.5\n",
            "('mathematical', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'using')-0.09090909090909091\n",
            "('using', 'mfcc')-1.0\n",
            "('mfcc', 'dtw')-1.0\n",
            "('dtw', 'extract')-0.25\n",
            "('extract', 'match')-1.0\n",
            "('match', 'features')-0.5\n",
            "('features', 'improve')-0.5\n",
            "('improve', 'accuracy')-1.0\n",
            "('accuracy', 'better')-0.5\n",
            "('better', 'performance')-0.25\n",
            "('performance', 'abstract')-0.05\n",
            "('abstract', 'testing')-0.5\n",
            "('testing', 'natural')-0.010416666666666666\n",
            "('language', 'requirements')-0.2\n",
            "('requirements', 'standard')-0.3333333333333333\n",
            "('standard', 'approach')-0.09090909090909091\n",
            "('approach', 'system')-0.07142857142857142\n",
            "('system', 'acceptance')-1.0\n",
            "('acceptance', 'testing')-0.5\n",
            "('testing', 'this')-0.034482758620689655\n",
            "('this', 'test')-0.5\n",
            "('test', 'often')-0.5\n",
            "('often', 'performed')-1.0\n",
            "('performed', 'independent')-0.3333333333333333\n",
            "('independent', 'test')-0.5\n",
            "('test', 'organization')-1.0\n",
            "('organization', 'unfamiliar')-1.0\n",
            "('unfamiliar', 'application')-0.2\n",
            "('application', 'area')-1.0\n",
            "('area', 'the')-0.038461538461538464\n",
            "('the', 'things')-1.0\n",
            "('things', 'testers')-1.0\n",
            "('testers', 'go')-1.0\n",
            "('go', 'written')-1.0\n",
            "('written', 'requirements')-0.2\n",
            "('requirements', 'so')-0.5\n",
            "('so', 'abstract')-0.05\n",
            "('found', 'conversational')-1.0\n",
            "('conversational', 'partners')-1.0\n",
            "('partners', 'but')-1.0\n",
            "('but', 'also')-0.2\n",
            "('also', 'provides')-0.5\n",
            "('provides', 'us')-0.5\n",
            "('us', 'information')-0.045454545454545456\n",
            "('information', 'creative')-1.0\n",
            "('creative', 'making')-1.0\n",
            "('making', 'associations')-1.0\n",
            "('associations', 'storytelling')-1.0\n",
            "('storytelling', 'language')-0.009174311926605505\n",
            "('language', 'use')-0.08333333333333333\n",
            "('use', 'many')-0.2\n",
            "('many', 'subtleties')-1.0\n",
            "('subtleties', 'face')-0.2\n",
            "('face', 'multiparty')-1.0\n",
            "('multiparty', 'interaction')-1.0\n",
            "('interaction', 'added')-1.0\n",
            "('added', 'using')-0.09090909090909091\n",
            "('using', 'humor')-1.0\n",
            "('humor', 'persuade')-1.0\n",
            "('persuade', 'dominate')-1.0\n",
            "('dominate', 'soften')-1.0\n",
            "('soften', 'avoid')-0.3333333333333333\n",
            "('avoid', 'face')-0.2\n",
            "('face', 'threatening')-1.0\n",
            "('threatening', 'act')-1.0\n",
            "('act', 'abstract')-0.05\n",
            "('found', 'in')-0.11764705882352941\n",
            "('in', 'recent')-0.16666666666666666\n",
            "('recent', 'years')-0.125\n",
            "('years', 'machine')-0.058823529411764705\n",
            "('learning', 'ml')-1.0\n",
            "('ml', 'used')-0.09090909090909091\n",
            "('used', 'solve')-0.5\n",
            "('solve', 'complex')-1.0\n",
            "('complex', 'tasks')-0.0625\n",
            "('tasks', 'different')-0.25\n",
            "('different', 'disciplines')-1.0\n",
            "('disciplines', 'ranging')-1.0\n",
            "('ranging', 'data')-0.125\n",
            "('data', 'mining')-1.0\n",
            "('mining', 'information')-0.045454545454545456\n",
            "('information', 'we')-0.041666666666666664\n",
            "('we', 'argue')-0.16666666666666666\n",
            "('argue', 'manual')-0.3333333333333333\n",
            "('manual', 'automatic')-1.0\n",
            "('automatic', 'thesauruses')-0.3333333333333333\n",
            "('thesauruses', 'alternative')-1.0\n",
            "('alternative', 'resources')-0.2\n",
            "('resources', 'nlp')-0.022222222222222223\n",
            "('this', 'involves')-0.5\n",
            "('involves', 'radical')-1.0\n",
            "('radical', 'step')-1.0\n",
            "('step', 'interpreting')-1.0\n",
            "('interpreting', 'manual')-0.3333333333333333\n",
            "('manual', 'thesauruses')-0.3333333333333333\n",
            "('thesauruses', 'classifications')-1.0\n",
            "('classifications', 'words')-0.16666666666666666\n",
            "('words', 'rather')-0.3333333333333333\n",
            "('rather', 'word')-0.1111111111111111\n",
            "('word', 'senses')-0.5\n",
            "('senses', 'case')-0.5\n",
            "('case', 'made')-0.3333333333333333\n",
            "('made', 'the')-0.038461538461538464\n",
            "('the', 'range')-0.16666666666666666\n",
            "('range', 'roles')-0.5\n",
            "('roles', 'thesauruses')-0.3333333333333333\n",
            "('thesauruses', 'within')-0.2\n",
            "('within', 'nlp')-0.044444444444444446\n",
            "('nlp', 'briefly')-0.3333333333333333\n",
            "('briefly', 'presented')-0.5\n",
            "('presented', 'wasps')-1.0\n",
            "('wasps', 'thesaurus')-0.3333333333333333\n",
            "('thesaurus', 'introduced')-0.5\n",
            "('introduced', 'thesaurus')-0.3333333333333333\n",
            "('thesaurus', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'becoming')-1.0\n",
            "('becoming', 'urgent')-1.0\n",
            "('urgent', 'a')-0.25\n",
            "('a', 'range')-0.16666666666666666\n",
            "('range', 'evaluation')-0.1111111111111111\n",
            "('strategies', 'embedded')-1.0\n",
            "('embedded', 'within')-0.2\n",
            "('tasks', 'proposed')-1.0\n",
            "('proposed', 'introduction')-0.2\n",
            "('introduction', 'patterns')-0.2\n",
            "('patterns', 'music')-0.25\n",
            "('music', 'object')-0.5\n",
            "('object', 'intensive')-0.5\n",
            "('intensive', 'studies')-0.5\n",
            "('studies', 'past')-0.5\n",
            "('past', 'years')-0.25\n",
            "('years', 'one')-0.125\n",
            "('one', 'purposes')-0.5\n",
            "('purposes', 'analyzing')-0.5\n",
            "('analyzing', 'musical')-0.3333333333333333\n",
            "('musical', 'structure')-0.2\n",
            "('structure', 'form')-0.5\n",
            "('form', 'discover')-1.0\n",
            "('discover', 'patterns')-0.2\n",
            "('patterns', 'explicit')-1.0\n",
            "('explicit', 'implicit')-0.5\n",
            "('implicit', 'musical')-0.3333333333333333\n",
            "('musical', 'works')-0.5\n",
            "('works', 'simon')-1.0\n",
            "('simon', 'patterns')-0.2\n",
            "('patterns', 'comprise')-1.0\n",
            "('comprise', 'periodicity')-1.0\n",
            "('periodicity', 'make')-0.5\n",
            "('make', 'use')-0.16666666666666666\n",
            "('use', 'alphabets')-1.0\n",
            "('alphabets', 'compound')-0.3333333333333333\n",
            "('compound', 'made')-0.3333333333333333\n",
            "('made', 'subpatterns')-1.0\n",
            "('subpatterns', 'possess')-1.0\n",
            "('possess', 'phrase')-1.0\n",
            "('phrase', 'structure')-0.2\n",
            "('structure', 'various')-0.125\n",
            "('various', 'forms')-0.3333333333333333\n",
            "('forms', 'punctuation')-1.0\n",
            "('punctuation', 'traditionally')-1.0\n",
            "('traditionally', 'composers')-1.0\n",
            "('composers', 'employed')-0.5\n",
            "('employed', 'pattern')-0.5\n",
            "('pattern', 'propagation')-1.0\n",
            "('propagation', 'intuitively')-0.5\n",
            "('intuitively', 'algorithmic')-1.0\n",
            "('algorithmic', 'composition')-0.5\n",
            "('composition', 'techniques')-0.07142857142857142\n",
            "('techniques', 'allow')-1.0\n",
            "('allow', 'pattern')-0.5\n",
            "('propagation', 'formalized')-1.0\n",
            "('formalized', 'albeit')-1.0\n",
            "('albeit', 'high')-0.3333333333333333\n",
            "('level', 'during')-0.5\n",
            "('during', 'composition')-0.5\n",
            "('composition', 'musical')-0.3333333333333333\n",
            "('musical', 'patterns')-0.2\n",
            "('patterns', 'evolve')-1.0\n",
            "('evolve', 'according')-1.0\n",
            "('according', 'rules')-0.5\n",
            "('rules', 'constraints')-0.5\n",
            "('constraints', 'specied')-1.0\n",
            "('specied', 'design')-0.25\n",
            "('design', 'stage')-1.0\n",
            "('stage', 'in')-0.058823529411764705\n",
            "('in', 'jazz')-1.0\n",
            "('jazz', 'improvisation')-1.0\n",
            "('improvisation', 'musician')-1.0\n",
            "('musician', 'invents')-1.0\n",
            "('invents', 'solo')-0.5\n",
            "('solo', 'guided')-1.0\n",
            "('guided', 'progression')-0.5\n",
            "('progression', 'chords')-1.0\n",
            "('chords', 'changes')-1.0\n",
            "('changes', 'one')-0.125\n",
            "('one', 'approach')-0.09090909090909091\n",
            "('approach', 'learn')-0.5\n",
            "('learn', 'improvising')-1.0\n",
            "('improvising', 'memorize')-1.0\n",
            "('memorize', 'patterns')-0.2\n",
            "('patterns', 'short')-0.5\n",
            "('short', 'chunks')-0.5\n",
            "('chunks', 'music')-0.25\n",
            "('music', 'sub')-1.0\n",
            "('sub', 'progressions')-1.0\n",
            "('progressions', 'concatenate')-1.0\n",
            "('concatenate', 'form')-0.5\n",
            "('form', 'whole')-0.5\n",
            "('whole', 'solo')-0.5\n",
            "('solo', 'ts')-1.0\n",
            "('ts', 'whole')-0.5\n",
            "('whole', 'progression')-0.5\n",
            "('progression', 'one')-0.125\n",
            "('one', 'abstract')-0.05\n",
            "('abstract', 'many')-0.2\n",
            "('many', 'information')-0.045454545454545456\n",
            "('retrieval', 'ir')-1.0\n",
            "('ir', 'systems')-0.0625\n",
            "('systems', 'retrieve')-1.0\n",
            "('retrieve', 'relevant')-0.3333333333333333\n",
            "('relevant', 'documents')-0.4\n",
            "('documents', 'based')-0.07692307692307693\n",
            "('based', 'exact')-1.0\n",
            "('exact', 'matching')-0.5\n",
            "('matching', 'keywords')-0.5\n",
            "('keywords', 'query')-0.3333333333333333\n",
            "('query', 'documents')-0.2\n",
            "('documents', 'this')-0.034482758620689655\n",
            "('this', 'method')-0.2\n",
            "('method', 'degrades')-1.0\n",
            "('degrades', 'precision')-0.5\n",
            "('precision', 'rate')-1.0\n",
            "('rate', 'in')-0.058823529411764705\n",
            "('in', 'order')-0.5\n",
            "('order', 'solve')-0.5\n",
            "('solve', 'problem')-1.0\n",
            "('problem', 'collected')-1.0\n",
            "('collected', 'semantically')-0.5\n",
            "('semantically', 'related')-0.5\n",
            "('related', 'words')-0.16666666666666666\n",
            "('words', 'assigned')-1.0\n",
            "('assigned', 'semantic')-0.125\n",
            "('semantic', 'relationships')-0.5\n",
            "('relationships', 'used')-0.09090909090909091\n",
            "('used', 'general')-0.3333333333333333\n",
            "('general', 'thesaurus')-0.3333333333333333\n",
            "('thesaurus', 'special')-0.5\n",
            "('special', 'relationship')-1.0\n",
            "('relationship', 'called')-0.5\n",
            "('called', 'keyfact')-0.25\n",
            "('keyfact', 'term')-0.5\n",
            "('term', 'ft')-1.0\n",
            "('ft', 'manually')-0.5\n",
            "('manually', 'in')-0.058823529411764705\n",
            "('in', 'addition')-1.0\n",
            "('addition', 'semantic')-0.125\n",
            "('semantic', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'automatically')-0.3333333333333333\n",
            "('automatically', 'constructed')-1.0\n",
            "('constructed', 'statistic')-1.0\n",
            "('statistic', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'based')-0.07692307692307693\n",
            "('based', 'concept')-0.5\n",
            "('concept', 'mutual')-1.0\n",
            "('mutual', 'information')-0.045454545454545456\n",
            "('information', 'keyfact')-0.25\n",
            "('keyfact', 'extended')-1.0\n",
            "('extended', 'concept')-0.5\n",
            "('concept', 'keyword')-1.0\n",
            "('keyword', 'represented')-1.0\n",
            "('represented', 'noun')-0.5\n",
            "('noun', 'compound')-0.3333333333333333\n",
            "('compound', 'noun')-0.5\n",
            "('noun', 'keyfact')-0.25\n",
            "('keyfact', 'verb')-1.0\n",
            "('verb', 'adjective')-1.0\n",
            "('adjective', 'including')-0.125\n",
            "('including', 'subject')-0.5\n",
            "('subject', 'object')-0.5\n",
            "('object', 'term')-0.5\n",
            "('term', 'we')-0.041666666666666664\n",
            "('we', 'first')-0.3333333333333333\n",
            "('first', 'retrieved')-1.0\n",
            "('retrieved', 'relevant')-0.3333333333333333\n",
            "('documents', 'original')-1.0\n",
            "('original', 'query')-0.3333333333333333\n",
            "('query', 'using')-0.09090909090909091\n",
            "('using', 'tf')-1.0\n",
            "('tf', 'idf')-1.0\n",
            "('idf', 'weighting')-0.5\n",
            "('weighting', 'formula')-1.0\n",
            "('formula', 'expanded')-0.5\n",
            "('expanded', 'query')-0.3333333333333333\n",
            "('query', 'including')-0.125\n",
            "('including', 'keyfacts')-1.0\n",
            "('keyfacts', 'used')-0.09090909090909091\n",
            "('used', 'second')-1.0\n",
            "('second', 'document')-0.2\n",
            "('document', 'ranking')-1.0\n",
            "('ranking', 'word')-0.1111111111111111\n",
            "('word', 'sense')-0.25\n",
            "('sense', 'disambiguating')-1.0\n",
            "('disambiguating', 'so')-0.5\n",
            "('so', 'made')-0.3333333333333333\n",
            "('made', 'improvement')-0.25\n",
            "('improvement', 'precision')-0.5\n",
            "('rate', 'using')-0.09090909090909091\n",
            "('using', 'keyfact')-0.25\n",
            "('keyfact', 'network')-0.16666666666666666\n",
            "('network', 'paper')-0.047619047619047616\n",
            "('paper', 'argue')-0.16666666666666666\n",
            "('argue', 'questionanswering')-1.0\n",
            "('questionanswering', 'qa')-0.3333333333333333\n",
            "('qa', 'technical')-1.0\n",
            "('technical', 'domains')-0.3333333333333333\n",
            "('domains', 'distinctly')-1.0\n",
            "('distinctly', 'different')-0.25\n",
            "('different', 'trec')-1.0\n",
            "('trec', 'based')-0.07692307692307693\n",
            "('based', 'qa')-0.6666666666666666\n",
            "('qa', 'web')-0.2\n",
            "('web', 'based')-0.15384615384615385\n",
            "('qa', 'can')-1.0\n",
            "('can', 'not')-1.0\n",
            "('not', 'benefit')-0.5\n",
            "('benefit', 'lom')-1.0\n",
            "('lom', 'data')-0.125\n",
            "('data', 'intensive')-0.5\n",
            "('intensive', 'approaches')-0.16666666666666666\n",
            "('approaches', 'universit')-1.0\n",
            "('universit', 'quot')-1.0\n",
            "('quot', 'des')-1.0\n",
            "('des', 'saarlandes')-1.0\n",
            "('saarlandes', 'proceedings')-1.0\n",
            "('proceedings', 'workshop')-1.0\n",
            "('workshop', 'uni')-1.0\n",
            "('uni', 'hamburg')-1.0\n",
            "('hamburg', 'de')-1.0\n",
            "('de', 'abstract')-0.05\n",
            "('found', 'abstract')-0.1\n",
            "('found', 'sri')-1.0\n",
            "('sri', 'developed')-0.2\n",
            "('developed', 'new')-0.16666666666666666\n",
            "('new', 'architecture')-0.25\n",
            "('architecture', 'integrating')-0.5\n",
            "('integrating', 'speech')-0.09090909090909091\n",
            "('speech', 'natural')-0.010416666666666666\n",
            "('processing', 'applies')-1.0\n",
            "('applies', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'constraints')-0.5\n",
            "('constraints', 'recognition')-0.08333333333333333\n",
            "('recognition', 'incrementally')-1.0\n",
            "('incrementally', 'expanding')-1.0\n",
            "('expanding', 'state')-0.125\n",
            "('state', 'transition')-1.0\n",
            "('transition', 'network')-0.16666666666666666\n",
            "('network', 'embodied')-1.0\n",
            "('embodied', 'unification')-1.0\n",
            "('unification', 'grammar')-0.5\n",
            "('grammar', 'we')-0.041666666666666664\n",
            "('we', 'compare')-1.0\n",
            "('compare', 'dynamic')-1.0\n",
            "('dynamic', 'gralnlnar')-1.0\n",
            "('gralnlnar', 'network')-0.16666666666666666\n",
            "('network', 'dgn')-1.0\n",
            "('dgn', 'approach')-0.09090909090909091\n",
            "('approach', 'this')-0.06896551724137931\n",
            "('this', 'chapter')-0.8\n",
            "('chapter', 'considers')-1.0\n",
            "('considers', 'revolution')-1.0\n",
            "('revolution', 'taken')-0.5\n",
            "('taken', 'place')-1.0\n",
            "('place', 'natural')-0.010416666666666666\n",
            "('processing', 'research')-0.08333333333333333\n",
            "('research', 'last')-0.25\n",
            "('last', 'five')-1.0\n",
            "('five', 'years')-0.125\n",
            "('years', 'it')-0.2\n",
            "('it', 'begins')-1.0\n",
            "('begins', 'providing')-0.5\n",
            "('providing', 'brief')-0.3333333333333333\n",
            "('brief', 'guide')-1.0\n",
            "('guide', 'structure')-0.2\n",
            "('structure', 'field')-0.2\n",
            "('field', 'presents')-0.3333333333333333\n",
            "('presents', 'caricature')-1.0\n",
            "('caricature', 'two')-0.3333333333333333\n",
            "('two', 'competing')-1.0\n",
            "('competing', 'paradigms')-1.0\n",
            "('paradigms', 'nlp')-0.022222222222222223\n",
            "('nlp', 'research')-0.16666666666666666\n",
            "('research', 'indicates')-1.0\n",
            "('indicates', 'reasons')-1.0\n",
            "('reasons', 'visual')-0.3333333333333333\n",
            "('visual', 'development')-0.1\n",
            "('development', 'environment')-0.5\n",
            "('environment', 'support')-0.3333333333333333\n",
            "('support', 'visual')-0.3333333333333333\n",
            "('visual', 'assembly')-1.0\n",
            "('assembly', 'execution')-0.5\n",
            "('execution', 'analysis')-0.09090909090909091\n",
            "('analysis', 'modular')-1.0\n",
            "('modular', 'natural')-0.010416666666666666\n",
            "('the', 'visual')-0.3333333333333333\n",
            "('visual', 'model')-0.14285714285714285\n",
            "('model', 'executable')-1.0\n",
            "('executable', 'data')-0.125\n",
            "('data', 'flow')-1.0\n",
            "('flow', 'program')-0.3333333333333333\n",
            "('program', 'graph')-0.3333333333333333\n",
            "('graph', 'automatically')-0.3333333333333333\n",
            "('automatically', 'synthesised')-1.0\n",
            "('synthesised', 'data')-0.125\n",
            "('data', 'dependency')-0.5\n",
            "('dependency', 'declarations')-1.0\n",
            "('declarations', 'language')-0.009174311926605505\n",
            "('processing', 'modules')-1.0\n",
            "('modules', 'the')-0.038461538461538464\n",
            "('the', 'graph')-0.3333333333333333\n",
            "('graph', 'in')-0.058823529411764705\n",
            "('in', 'chapter')-0.2\n",
            "('chapter', 'basic')-0.25\n",
            "('basic', 'uses')-0.5\n",
            "('uses', 'description')-0.25\n",
            "('logics', 'natural')-0.010416666666666666\n",
            "('processing', 'analysed')-1.0\n",
            "('analysed', 'together')-1.0\n",
            "('together', 'little')-1.0\n",
            "('little', 'bit')-1.0\n",
            "('bit', 'history')-0.25\n",
            "('history', 'role')-0.3333333333333333\n",
            "('role', 'description')-0.25\n",
            "('logics', 'current')-0.14285714285714285\n",
            "('state', 'art')-1.0\n",
            "('art', 'computational')-0.2\n",
            "('linguistics', 'pointed')-1.0\n",
            "('pointed', 'introduction')-0.2\n",
            "('introduction', 'since')-0.5\n",
            "('since', 'early')-1.0\n",
            "('early', 'days')-1.0\n",
            "('days', 'we')-0.041666666666666664\n",
            "('we', 'applied')-0.2\n",
            "('applied', 'structure')-0.2\n",
            "('structure', 'learning')-0.047619047619047616\n",
            "('learning', 'model')-0.14285714285714285\n",
            "('model', 'max')-1.0\n",
            "('max', 'margin')-1.0\n",
            "('margin', 'structure')-0.2\n",
            "('structure', 'mms')-1.0\n",
            "('mms', 'natural')-0.010416666666666666\n",
            "('tasks', 'aim')-1.0\n",
            "('aim', 'capture')-0.3333333333333333\n",
            "('capture', 'latent')-1.0\n",
            "('latent', 'relationships')-0.5\n",
            "('relationships', 'within')-0.2\n",
            "('within', 'output')-0.5\n",
            "('output', 'language')-0.009174311926605505\n",
            "('language', 'domain')-0.5\n",
            "('domain', 'we')-0.041666666666666664\n",
            "('we', 'formulate')-1.0\n",
            "('formulate', 'model')-0.14285714285714285\n",
            "('model', 'extension')-1.0\n",
            "('extension', 'multi')-1.0\n",
            "('multi', 'class')-0.3333333333333333\n",
            "('class', 'support')-0.3333333333333333\n",
            "('support', 'vector')-1.0\n",
            "('vector', 'machine')-0.058823529411764705\n",
            "('machine', 'svm')-1.0\n",
            "('svm', 'present')-0.25\n",
            "('present', 'mation')-1.0\n",
            "('mation', 'infrastructure')-0.5\n",
            "('infrastructure', 'digital')-0.3333333333333333\n",
            "('digital', 'libraries')-1.0\n",
            "('libraries', 'networked')-1.0\n",
            "('networked', 'services')-1.0\n",
            "('services', 'digital')-0.3333333333333333\n",
            "('digital', 'convergence')-1.0\n",
            "('convergence', 'intelligent')-0.5\n",
            "('intelligent', 'agents')-1.0\n",
            "('agents', 'this')-0.034482758620689655\n",
            "('this', 'attention')-1.0\n",
            "('attention', 'moving')-1.0\n",
            "('moving', 'natural')-0.010416666666666666\n",
            "('processing', 'along')-1.0\n",
            "('along', 'critical')-1.0\n",
            "('critical', 'path')-1.0\n",
            "('path', 'kinds')-0.5\n",
            "('kinds', 'novel')-1.0\n",
            "('novel', 'applications')-0.08333333333333333\n",
            "('applications', 'this')-0.06896551724137931\n",
            "('article', 'mention')-1.0\n",
            "('mention', 'number')-0.2\n",
            "('number', 'successful')-1.0\n",
            "('successful', 'applications')-0.08333333333333333\n",
            "('applications', 'natural')-0.010416666666666666\n",
            "('nlp', 'over')-1.0\n",
            "('over', 'last')-0.25\n",
            "('last', 'years')-0.125\n",
            "('years', 'number')-0.2\n",
            "('number', 'areas')-1.0\n",
            "('areas', 'natural')-0.010416666666666666\n",
            "('processing', 'begun')-1.0\n",
            "('begun', 'applying')-0.5\n",
            "('applying', 'graph')-0.3333333333333333\n",
            "('graph', 'based')-0.07692307692307693\n",
            "('based', 'techniques')-0.07142857142857142\n",
            "('techniques', 'these')-1.0\n",
            "('these', 'include')-0.5\n",
            "('include', 'among')-0.3333333333333333\n",
            "('among', 'others')-1.0\n",
            "('others', 'text')-0.09090909090909091\n",
            "('text', 'summarization')-1.0\n",
            "('summarization', 'syntactic')-0.16666666666666666\n",
            "('syntactic', 'parsing')-0.5\n",
            "('parsing', 'word')-0.2222222222222222\n",
            "('sense', 'disambiguation')-1.0\n",
            "('disambiguation', 'ontology')-1.0\n",
            "('ontology', 'construction')-1.0\n",
            "('construction', 'sentiment')-1.0\n",
            "('sentiment', 'subjectivity')-1.0\n",
            "('subjectivity', 'analysis')-0.09090909090909091\n",
            "('text', 'clustering')-1.0\n",
            "('clustering', 'in')-0.058823529411764705\n",
            "('research', 'results')-0.42857142857142855\n",
            "('results', 'software')-0.25\n",
            "('software', 'engineering')-0.5\n",
            "('engineering', 'software')-0.25\n",
            "('software', 'technology')-0.3333333333333333\n",
            "('technology', 'often')-0.5\n",
            "('often', 'neglected')-1.0\n",
            "('neglected', 'kernelized')-0.5\n",
            "('kernelized', 'sorting')-1.0\n",
            "('sorting', 'increase')-1.0\n",
            "('increase', 'robustness')-1.0\n",
            "('robustness', 'performance')-0.25\n",
            "('performance', 'several')-0.2\n",
            "('several', 'natural')-0.010416666666666666\n",
            "('tasks', 'document')-0.2\n",
            "('document', 'matching')-0.5\n",
            "('matching', 'parallel')-0.5\n",
            "('parallel', 'comparable')-1.0\n",
            "('comparable', 'corpora')-1.0\n",
            "('corpora', 'machine')-0.058823529411764705\n",
            "('machine', 'transliteration')-1.0\n",
            "('transliteration', 'even')-0.5\n",
            "('even', 'image')-1.0\n",
            "('image', 'processing')-0.011764705882352941\n",
            "('processing', 'empirically')-1.0\n",
            "('empirically', 'show')-1.0\n",
            "('show', 'tasks')-0.0625\n",
            "('tasks', 'semi')-1.0\n",
            "('semi', 'supervised')-1.0\n",
            "('supervised', 'variant')-1.0\n",
            "('variant', 'kernelized')-0.5\n",
            "('kernelized', 'structured')-0.5\n",
            "('structured', 'in')-0.058823529411764705\n",
            "('in', 'words')-0.16666666666666666\n",
            "('words', 'statistical')-0.125\n",
            "('processing', 'need')-0.16666666666666666\n",
            "('need', 'sophisticated')-0.5\n",
            "('sophisticated', 'statistical')-0.125\n",
            "('statistical', 'model')-0.14285714285714285\n",
            "('model', 'basic')-0.25\n",
            "('basic', 'elements')-0.3333333333333333\n",
            "('elements', 'words')-0.16666666666666666\n",
            "('words', 'phrases')-1.0\n",
            "('phrases', 'combined')-1.0\n",
            "('combined', 'structural')-1.0\n",
            "('structural', 'modeling')-0.3333333333333333\n",
            "('modeling', 'syntactic')-0.16666666666666666\n",
            "('parsing', 'dependency')-0.5\n",
            "('dependency', 'analysis')-0.09090909090909091\n",
            "('analysis', 'since')-0.5\n",
            "('since', 'basic')-0.25\n",
            "('basic', 'property')-1.0\n",
            "('property', 'elements')-0.3333333333333333\n",
            "('elements', 'in')-0.058823529411764705\n",
            "('describe', 'framework')-0.5\n",
            "('framework', 'developing')-1.0\n",
            "('developing', 'probabilistic')-0.3333333333333333\n",
            "('probabilistic', 'classifiers')-1.0\n",
            "('classifiers', 'natural')-0.010416666666666666\n",
            "('processing', 'our')-0.5\n",
            "('our', 'focus')-0.16666666666666666\n",
            "('focus', 'formulating')-1.0\n",
            "('formulating', 'models')-0.25\n",
            "('models', 'capture')-0.3333333333333333\n",
            "('capture', 'important')-0.3333333333333333\n",
            "('important', 'interdependencies')-1.0\n",
            "('interdependencies', 'among')-0.3333333333333333\n",
            "('among', 'features')-0.5\n",
            "('features', 'avoid')-0.3333333333333333\n",
            "('avoid', 'overfitting')-1.0\n",
            "('overfitting', 'data')-0.125\n",
            "('data', 'also')-0.2\n",
            "('also', 'characterizing')-1.0\n",
            "('characterizing', 'data')-0.125\n",
            "('data', 'well')-1.0\n",
            "('well', 'the')-0.038461538461538464\n",
            "('the', 'class')-0.3333333333333333\n",
            "('class', 'many')-0.2\n",
            "('many', 'natural')-0.010416666666666666\n",
            "('techniques', 'used')-0.09090909090909091\n",
            "('used', 'information')-0.045454545454545456\n",
            "('retrieval', 'the')-0.07692307692307693\n",
            "('the', 'results')-0.14285714285714285\n",
            "('results', 'encouraging')-1.0\n",
            "('encouraging', 'simple')-0.5\n",
            "('simple', 'methods')-0.3333333333333333\n",
            "('methods', 'stopwording')-1.0\n",
            "('stopwording', 'porter')-1.0\n",
            "('porter', 'style')-1.0\n",
            "('style', 'stemming')-1.0\n",
            "('stemming', 'etc')-1.0\n",
            "('etc', 'usually')-1.0\n",
            "('usually', 'yield')-1.0\n",
            "('yield', 'significant')-0.3333333333333333\n",
            "('significant', 'improvements')-1.0\n",
            "('improvements', 'higher')-1.0\n",
            "('higher', 'level')-0.3333333333333333\n",
            "('level', 'processing')-0.011764705882352941\n",
            "('processing', 'chunking')-0.5\n",
            "('chunking', 'parsing')-0.25\n",
            "('disambiguation', 'abstract')-0.05\n",
            "('abstract', 'this')-0.06896551724137931\n",
            "('paper', 'explains')-1.0\n",
            "('explains', 'information')-0.045454545454545456\n",
            "('retrieval', 'using')-0.09090909090909091\n",
            "('processing', 'malayalam')-1.0\n",
            "('malayalam', 'language')-0.009174311926605505\n",
            "('language', 'basic')-0.25\n",
            "('basic', 'state')-0.125\n",
            "('art', 'plan')-0.5\n",
            "('plan', 'recognition')-0.3333333333333333\n",
            "('recognition', 'systems')-0.125\n",
            "('systems', 'this')-0.06896551724137931\n",
            "('paper', 'outline')-1.0\n",
            "('outline', 'relations')-1.0\n",
            "('relations', 'natural')-0.020833333333333332\n",
            "('nlp', 'plan')-0.5\n",
            "('recognition', 'pr')-0.5\n",
            "('pr', 'argue')-0.3333333333333333\n",
            "('argue', 'effectively')-0.6666666666666666\n",
            "('effectively', 'inform')-0.6666666666666666\n",
            "('inform', 'focus')-0.3333333333333333\n",
            "('focus', 'key')-0.6666666666666666\n",
            "('key', 'recent')-0.3333333333333333\n",
            "('results', 'nlp')-0.044444444444444446\n",
            "('nlp', 'argue')-0.3333333333333333\n",
            "('argue', 'applicability')-1.0\n",
            "('applicability', 'pr')-0.5\n",
            "('pr', 'state')-0.125\n",
            "('pr', 'information')-0.045454545454545456\n",
            "('retrieval', 'process')-0.125\n",
            "('process', 'finding')-1.0\n",
            "('finding', 'documents')-0.2\n",
            "('documents', 'document')-0.2\n",
            "('document', 'collection')-0.5\n",
            "('collection', 'satisfies')-1.0\n",
            "('satisfies', 'information')-0.045454545454545456\n",
            "('information', 'need')-0.16666666666666666\n",
            "('need', 'user')-0.5\n",
            "('user', 'the')-0.038461538461538464\n",
            "('the', 'documents')-0.2\n",
            "('documents', 'natural')-0.010416666666666666\n",
            "('language', 'constructs')-1.0\n",
            "('constructs', 'motivation')-0.5\n",
            "('motivation', 'work')-0.125\n",
            "('work', 'investigate')-1.0\n",
            "('investigate', 'natural')-0.010416666666666666\n",
            "('processing', 'used')-0.09090909090909091\n",
            "('used', 'improve')-0.5\n",
            "('improve', 'logic')-0.16666666666666666\n",
            "('logic', 'programming')-1.0\n",
            "('programming', 'within')-0.2\n",
            "('within', 'natural')-0.010416666666666666\n",
            "('language', 'research')-0.08333333333333333\n",
            "('research', 'machine')-0.058823529411764705\n",
            "('learning', 'point')-0.25\n",
            "('point', 'opportunities')-0.3333333333333333\n",
            "('opportunities', 'induction')-1.0\n",
            "('induction', 'linguistic')-0.14285714285714285\n",
            "('knowledge', 'within')-0.2\n",
            "('within', 'logic')-0.16666666666666666\n",
            "('programming', 'keywords')-0.5\n",
            "('keywords', 'inductive')-1.0\n",
            "('inductive', 'logic')-0.16666666666666666\n",
            "('programming', 'natural')-0.010416666666666666\n",
            "('processing', 'logic')-0.3333333333333333\n",
            "('programming', 'machine')-0.058823529411764705\n",
            "('learning', 'introduction')-0.2\n",
            "('introduction', 'there')-1.0\n",
            "('there', 'what')-1.0\n",
            "('what', 'statistical')-0.125\n",
            "('statistical', 'method')-0.2\n",
            "('method', 'used')-0.09090909090909091\n",
            "('used', 'natural')-0.010416666666666666\n",
            "('nlp', 'in')-0.058823529411764705\n",
            "('paper', 'start')-1.0\n",
            "('start', 'definition')-1.0\n",
            "('definition', 'nlp')-0.022222222222222223\n",
            "('concerned', 'design')-0.25\n",
            "('design', 'implementation')-0.5\n",
            "('implementation', 'effective')-0.25\n",
            "('effective', 'natural')-0.010416666666666666\n",
            "('language', 'input')-1.0\n",
            "('input', 'output')-0.5\n",
            "('output', 'components')-0.5\n",
            "('components', 'computational')-0.2\n",
            "('computational', 'systems')-0.0625\n",
            "('systems', 'we')-0.041666666666666664\n",
            "('we', 'distinguish')-1.0\n",
            "('distinguish', 'three')-0.3333333333333333\n",
            "('three', 'in')-0.058823529411764705\n",
            "('in', 'report')-0.5\n",
            "('report', 'collaborative')-1.0\n",
            "('collaborative', 'work')-0.125\n",
            "('work', 'fields')-0.3333333333333333\n",
            "('fields', 'machine')-0.058823529411764705\n",
            "('ml', 'natural')-0.010416666666666666\n",
            "('nlp', 'presented')-0.5\n",
            "('presented', 'the')-0.038461538461538464\n",
            "('the', 'document')-0.2\n",
            "('document', 'structured')-0.5\n",
            "('structured', 'two')-0.3333333333333333\n",
            "('two', 'parts')-1.0\n",
            "('parts', 'the')-0.038461538461538464\n",
            "('the', 'first')-0.3333333333333333\n",
            "('first', 'part')-0.16666666666666666\n",
            "('part', 'includes')-0.3333333333333333\n",
            "('includes', 'superficial')-1.0\n",
            "('superficial', 'comprehensive')-1.0\n",
            "('comprehensive', 'survey')-1.0\n",
            "('survey', 'covering')-0.5\n",
            "('covering', 'state')-0.125\n",
            "('art', 'machine')-0.058823529411764705\n",
            "('learning', 'abstract')-0.05\n",
            "('this', 'thesis')-0.5\n",
            "('thesis', 'examines')-0.3333333333333333\n",
            "('examines', 'use')-0.08333333333333333\n",
            "('use', 'machine')-0.058823529411764705\n",
            "('techniques', 'various')-0.125\n",
            "('various', 'tasks')-0.0625\n",
            "('tasks', 'natural')-0.010416666666666666\n",
            "('processing', 'mainly')-1.0\n",
            "('mainly', 'task')-0.2\n",
            "('task', 'information')-0.045454545454545456\n",
            "('information', 'extraction')-0.6666666666666666\n",
            "('extraction', 'texts')-0.5\n",
            "('texts', 'the')-0.038461538461538464\n",
            "('the', 'objectives')-0.5\n",
            "('objectives', 'improvement')-0.25\n",
            "('improvement', 'adaptability')-1.0\n",
            "('adaptability', 'information')-0.045454545454545456\n",
            "('extraction', 'systems')-0.0625\n",
            "('systems', 'new')-0.16666666666666666\n",
            "('new', 'thematic')-1.0\n",
            "('thematic', 'mains')-1.0\n",
            "('mains', 'even')-0.5\n",
            "('even', 'this')-0.034482758620689655\n",
            "('chapter', 'examines')-0.6666666666666666\n",
            "('examines', 'application')-0.4\n",
            "('application', 'natural')-0.020833333333333332\n",
            "('processing', 'computerassisted')-1.0\n",
            "('computerassisted', 'language')-0.01834862385321101\n",
            "('language', 'learning')-0.09523809523809523\n",
            "('learning', 'including')-0.25\n",
            "('including', 'history')-0.5\n",
            "('history', 'work')-0.25\n",
            "('work', 'field')-0.4\n",
            "('field', 'last')-0.5\n",
            "('last', 'thirtyfive')-1.0\n",
            "('thirtyfive', 'years')-0.25\n",
            "('years', 'focus')-0.3333333333333333\n",
            "('focus', 'current')-0.2857142857142857\n",
            "('current', 'developments')-1.0\n",
            "('developments', 'opportunities')-0.6666666666666666\n",
            "('opportunities', 'traditional')-0.3333333333333333\n",
            "('traditional', 'approaches')-0.16666666666666666\n",
            "('approaches', 'tointerpretation')-1.0\n",
            "('tointerpretation', 'natural')-0.010416666666666666\n",
            "('processing', 'typically')-0.5\n",
            "('typically', 'fall')-0.5\n",
            "('fall', 'one')-0.125\n",
            "('one', 'three')-0.3333333333333333\n",
            "('three', 'classes')-0.3333333333333333\n",
            "('classes', 'syntax')-0.5\n",
            "('syntax', 'driven')-0.6666666666666666\n",
            "('driven', 'semantics')-0.5\n",
            "('semantics', 'driven')-0.3333333333333333\n",
            "('driven', 'frame')-1.0\n",
            "('frame', 'task')-0.2\n",
            "('task', 'based')-0.07692307692307693\n",
            "('based', 'syntax')-0.5\n",
            "('driven', 'approaches')-0.16666666666666666\n",
            "('approaches', 'use')-0.08333333333333333\n",
            "('use', 'domain')-0.25\n",
            "('domain', 'independent')-0.3333333333333333\n",
            "('independent', 'grammar')-0.5\n",
            "('grammar', 'drive')-0.5\n",
            "('drive', 'interpretation')-0.3333333333333333\n",
            "('interpretation', 'process')-0.125\n",
            "('process', 'produce')-1.0\n",
            "('produce', 'global')-0.5\n",
            "('global', 'parse')-1.0\n",
            "('parse', 'natural')-0.010416666666666666\n",
            "('nlp', 'large')-0.5\n",
            "('large', 'diverse')-1.0\n",
            "('diverse', 'subtopic')-1.0\n",
            "('subtopic', 'artificial')-0.5\n",
            "('artificial', 'intelligence')-1.0\n",
            "('intelligence', 'as')-1.0\n",
            "('as', 'result')-1.0\n",
            "('result', 'nlp')-0.022222222222222223\n",
            "('nlp', 'many')-0.2\n",
            "('many', 'subtopics')-1.0\n",
            "('subtopics', 'including')-0.125\n",
            "('including', 'optical')-1.0\n",
            "('optical', 'character')-1.0\n",
            "('character', 'recognition')-0.08333333333333333\n",
            "('recognition', 'text')-0.09090909090909091\n",
            "('text', 'speech')-0.09090909090909091\n",
            "('speech', 'translators')-1.0\n",
            "('translators', 'foreign')-1.0\n",
            "('foreign', 'language')-0.009174311926605505\n",
            "('language', 'reading')-1.0\n",
            "('reading', 'writing')-1.0\n",
            "('writing', 'aids')-1.0\n",
            "('aids', 'machine')-0.058823529411764705\n",
            "('translation', 'speech')-0.09090909090909091\n",
            "('recognition', 'probabilistic')-0.3333333333333333\n",
            "('probabilistic', 'finite')-1.0\n",
            "('finite', 'state')-0.125\n",
            "('state', 'string')-1.0\n",
            "('string', 'transducers')-1.0\n",
            "('transducers', 'fsts')-0.5\n",
            "('fsts', 'extremely')-1.0\n",
            "('extremely', 'popular')-1.0\n",
            "('popular', 'natural')-0.010416666666666666\n",
            "('processing', 'due')-1.0\n",
            "('due', 'powerful')-0.5\n",
            "('powerful', 'generic')-0.3333333333333333\n",
            "('generic', 'methods')-0.3333333333333333\n",
            "('methods', 'applying')-0.5\n",
            "('applying', 'composing')-1.0\n",
            "('composing', 'learning')-0.047619047619047616\n",
            "('learning', 'unfortunately')-1.0\n",
            "('unfortunately', 'fsts')-0.5\n",
            "('fsts', 'good')-0.5\n",
            "('good', 'fit')-1.0\n",
            "('fit', 'much')-0.5\n",
            "('much', 'current')-0.14285714285714285\n",
            "('current', 'work')-0.125\n",
            "('work', 'probabilistic')-0.3333333333333333\n",
            "('probabilistic', 'modeling')-0.3333333333333333\n",
            "('modeling', 'machine')-0.058823529411764705\n",
            "('machine', 'abstract')-0.05\n",
            "('abstract', 'in')-0.058823529411764705\n",
            "('in', 'special')-0.5\n",
            "('special', 'issue')-0.3333333333333333\n",
            "('issue', 'tal')-1.0\n",
            "('tal', 'look')-1.0\n",
            "('look', 'fundamental')-0.5\n",
            "('fundamental', 'principles')-0.3333333333333333\n",
            "('principles', 'underlying')-1.0\n",
            "('underlying', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'natural')-0.010416666666666666\n",
            "('we', 'adopt')-1.0\n",
            "('adopt', 'global')-0.5\n",
            "('global', 'point')-0.25\n",
            "('point', 'view')-1.0\n",
            "('view', 'goes')-1.0\n",
            "('goes', 'beyond')-1.0\n",
            "('beyond', 'horizon')-1.0\n",
            "('horizon', 'single')-0.3333333333333333\n",
            "('single', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'campaign')-1.0\n",
            "('campaign', 'particular')-0.2\n",
            "('particular', 'protocol')-1.0\n",
            "('protocol', 'after')-0.5\n",
            "('after', 'brief')-0.3333333333333333\n",
            "('brief', 'review')-0.5\n",
            "('review', 'history')-0.25\n",
            "('history', 'terminology')-1.0\n",
            "('terminology', 'abstract')-0.05\n",
            "('found', 'natural')-0.010416666666666666\n",
            "('systems', 'nlp')-0.022222222222222223\n",
            "('nlp', 'extract')-0.25\n",
            "('extract', 'clinical')-1.0\n",
            "('clinical', 'information')-0.045454545454545456\n",
            "('information', 'textual')-1.0\n",
            "('textual', 'reports')-1.0\n",
            "('reports', 'shown')-0.5\n",
            "('shown', 'effective')-0.25\n",
            "('effective', 'limited')-0.25\n",
            "('limited', 'domains')-0.3333333333333333\n",
            "('domains', 'particular')-0.2\n",
            "('particular', 'applications')-0.08333333333333333\n",
            "('applications', 'because')-1.0\n",
            "('because', 'nlp')-0.022222222222222223\n",
            "('system', 'typically')-0.5\n",
            "('typically', 'requires')-1.0\n",
            "('requires', 'substantial')-1.0\n",
            "('substantial', 'resources')-0.2\n",
            "('resources', 'develop')-1.0\n",
            "('develop', 'beneficial')-1.0\n",
            "('beneficial', 'designed')-1.0\n",
            "('designed', 'easily')-1.0\n",
            "('easily', 'facts')-1.0\n",
            "('facts', 'forms')-0.3333333333333333\n",
            "('forms', 'link')-1.0\n",
            "('link', 'ie')-1.0\n",
            "('ie', 'recent')-0.16666666666666666\n",
            "('programming', 'prolog')-0.5\n",
            "('prolog', 'we')-0.041666666666666664\n",
            "('describe', 'single')-0.3333333333333333\n",
            "('single', 'convolutional')-1.0\n",
            "('convolutional', 'neural')-0.3333333333333333\n",
            "('architecture', 'given')-0.2\n",
            "('given', 'sentence')-0.5\n",
            "('sentence', 'outputs')-1.0\n",
            "('outputs', 'host')-1.0\n",
            "('host', 'language')-0.009174311926605505\n",
            "('processing', 'predictions')-1.0\n",
            "('predictions', 'part')-0.16666666666666666\n",
            "('speech', 'tags')-0.5\n",
            "('tags', 'chunks')-0.5\n",
            "('chunks', 'named')-0.5\n",
            "('entity', 'tags')-0.5\n",
            "('tags', 'semantic')-0.125\n",
            "('semantic', 'roles')-0.5\n",
            "('roles', 'semantically')-0.5\n",
            "('semantically', 'similar')-0.5\n",
            "('similar', 'words')-0.16666666666666666\n",
            "('words', 'likelihood')-0.5\n",
            "('likelihood', 'sentence')-0.5\n",
            "('sentence', 'makes')-0.5\n",
            "('makes', 'sense')-0.08333333333333333\n",
            "('sense', 'grammatically')-1.0\n",
            "('grammatically', 'we')-0.041666666666666664\n",
            "('we', 'developed')-0.2\n",
            "('developed', 'prototype')-1.0\n",
            "('prototype', 'information')-0.045454545454545456\n",
            "('system', 'uses')-0.5\n",
            "('uses', 'advanced')-1.0\n",
            "('advanced', 'natural')-0.010416666666666666\n",
            "('techniques', 'enhance')-1.0\n",
            "('enhance', 'effectiveness')-1.0\n",
            "('effectiveness', 'traditional')-0.3333333333333333\n",
            "('traditional', 'key')-0.3333333333333333\n",
            "('key', 'word')-0.1111111111111111\n",
            "('word', 'based')-0.07692307692307693\n",
            "('based', 'document')-0.2\n",
            "('document', 'retrieval')-0.1\n",
            "('the', 'backbone')-1.0\n",
            "('backbone', 'system')-0.07142857142857142\n",
            "('system', 'statistical')-0.125\n",
            "('statistical', 'retrieval')-0.1\n",
            "('retrieval', 'engine')-1.0\n",
            "('engine', 'performs')-0.5\n",
            "('performs', 'automated')-0.2\n",
            "('automated', 'indexing')-1.0\n",
            "('indexing', 'abstract')-0.05\n",
            "('paper', 'discuss')-0.5\n",
            "('several', 'issues')-0.5\n",
            "('issues', 'requirements')-0.2\n",
            "('requirements', 'enabling')-1.0\n",
            "('enabling', 'natural')-0.010416666666666666\n",
            "('systems', 'become')-1.0\n",
            "('become', 'context')-0.5\n",
            "('context', 'adaptive')-1.0\n",
            "('adaptive', 'given')-0.2\n",
            "('given', 'fact')-1.0\n",
            "('fact', 'emerging')-1.0\n",
            "('emerging', 'systems')-0.0625\n",
            "('systems', 'feature')-1.0\n",
            "('feature', 'speaker')-1.0\n",
            "('speaker', 'independent')-0.3333333333333333\n",
            "('independent', 'continuous')-1.0\n",
            "('continuous', 'speech')-0.09090909090909091\n",
            "('recognition', 'restricted')-1.0\n",
            "('restricted', 'individual')-1.0\n",
            "('individual', 'domains')-0.3333333333333333\n",
            "('domains', 'equipped')-1.0\n",
            "('equipped', 'syntactic')-0.16666666666666666\n",
            "('syntactic', 'in')-0.058823529411764705\n",
            "('in', 'fall')-0.5\n",
            "('fall', 'i')-1.0\n",
            "('i', 'introduced')-0.5\n",
            "('introduced', 'new')-0.16666666666666666\n",
            "('new', 'course')-1.0\n",
            "('course', 'called')-0.5\n",
            "('called', 'applied')-0.2\n",
            "('applied', 'natural')-0.010416666666666666\n",
            "('processing', 'students')-1.0\n",
            "('students', 'acquire')-0.5\n",
            "('acquire', 'understanding')-0.16666666666666666\n",
            "('understanding', 'text')-0.09090909090909091\n",
            "('analysis', 'techniques')-0.07142857142857142\n",
            "('techniques', 'currently')-0.5\n",
            "('currently', 'feasible')-1.0\n",
            "('feasible', 'practical')-0.3333333333333333\n",
            "('practical', 'applications')-0.08333333333333333\n",
            "('applications', 'abstract')-0.05\n",
            "('processing', 'study')-0.5\n",
            "('study', 'mathematical')-0.5\n",
            "('mathematical', 'computational')-0.2\n",
            "('computational', 'modelling')-1.0\n",
            "('modelling', 'various')-0.125\n",
            "('various', 'aspects')-0.5\n",
            "('aspects', 'language')-0.009174311926605505\n",
            "('language', 'improvement')-0.25\n",
            "('improvement', 'wide')-0.3333333333333333\n",
            "('wide', 'range')-0.3333333333333333\n",
            "('range', 'systems')-0.0625\n",
            "('systems', 'natural')-0.010416666666666666\n",
            "('language', 'language')-0.009174311926605505\n",
            "('language', 'arises')-1.0\n",
            "('arises', 'innate')-1.0\n",
            "('innate', 'facility')-1.0\n",
            "('facility', 'language')-0.009174311926605505\n",
            "('language', 'possessed')-1.0\n",
            "('possessed', 'human')-0.5\n",
            "('human', 'intellect')-1.0\n",
            "('intellect', 'may')-0.3333333333333333\n",
            "('may', 'natural')-0.010416666666666666\n",
            "('nlp', 'branch')-1.0\n",
            "('branch', 'artificial')-0.5\n",
            "('intelligence', 'includes')-0.3333333333333333\n",
            "('includes', 'speech')-0.09090909090909091\n",
            "('speech', 'synthesis')-1.0\n",
            "('synthesis', 'speech')-0.09090909090909091\n",
            "('recognition', 'machine')-0.058823529411764705\n",
            "('translation', 'natural')-0.010416666666666666\n",
            "('processing', 'wide')-0.3333333333333333\n",
            "('range', 'applications')-0.08333333333333333\n",
            "('applications', 'indian')-0.5\n",
            "('indian', 'context')-0.5\n",
            "('context', 'most')-1.0\n",
            "('most', 'rural')-1.0\n",
            "('rural', 'indian')-0.5\n",
            "('indian', 'community')-0.5\n",
            "('community', 'unable')-1.0\n",
            "('unable', 'make')-0.5\n",
            "('use', 'an')-0.5\n",
            "('an', 'evaluation')-0.1111111111111111\n",
            "('evaluation', 'lolita')-0.3333333333333333\n",
            "('lolita', 'related')-0.5\n",
            "('related', 'natural')-0.010416666666666666\n",
            "('systems', 'paul')-1.0\n",
            "('paul', 'callaghan')-1.0\n",
            "('callaghan', 'submitted')-1.0\n",
            "('submitted', 'university')-1.0\n",
            "('university', 'durham')-1.0\n",
            "('durham', 'degree')-1.0\n",
            "('degree', 'ph')-1.0\n",
            "('ph', 'd')-1.0\n",
            "('d', 'august')-1.0\n",
            "('august', 'this')-0.034482758620689655\n",
            "('this', 'research')-0.08333333333333333\n",
            "('research', 'addresses')-0.5\n",
            "('addresses', 'question')-1.0\n",
            "('question', 'evaluate')-1.0\n",
            "('evaluate', 'systems')-0.0625\n",
            "('systems', 'like')-0.3333333333333333\n",
            "('like', 'lolita')-0.3333333333333333\n",
            "('lolita', 'lolita')-0.3333333333333333\n",
            "('lolita', 'natural')-0.010416666666666666\n",
            "('natural', 'previous')-1.0\n",
            "('previous', 'work')-0.125\n",
            "('work', 'demonstrated')-1.0\n",
            "('demonstrated', 'web')-0.2\n",
            "('web', 'counts')-0.5\n",
            "('counts', 'used')-0.09090909090909091\n",
            "('used', 'approximate')-1.0\n",
            "('approximate', 'bigram')-1.0\n",
            "('bigram', 'counts')-0.5\n",
            "('counts', 'suggesting')-1.0\n",
            "('suggesting', 'web')-0.2\n",
            "('based', 'frequencies')-1.0\n",
            "('frequencies', 'useful')-1.0\n",
            "('useful', 'wide')-0.3333333333333333\n",
            "('wide', 'variety')-0.5\n",
            "('variety', 'natural')-0.010416666666666666\n",
            "('tasks', 'however')-1.0\n",
            "('however', 'limited')-0.25\n",
            "('limited', 'number')-0.2\n",
            "('tasks', 'far')-0.5\n",
            "('far', 'tested')-1.0\n",
            "('tested', 'using')-0.09090909090909091\n",
            "('using', 'web')-0.2\n",
            "('web', 'scale')-1.0\n",
            "('scale', 'data')-0.125\n",
            "('data', 'sets')-1.0\n",
            "('sets', 'this')-0.034482758620689655\n",
            "('opportunities', 'introduction')-0.2\n",
            "('introduction', 'this')-0.034482758620689655\n",
            "('chapter', 'focuses')-0.3333333333333333\n",
            "('focuses', 'applications')-0.08333333333333333\n",
            "('paper', 'describes')-0.5\n",
            "('describes', 'natural')-0.010416666666666666\n",
            "('language', 'system')-0.07142857142857142\n",
            "('system', 'improves')-1.0\n",
            "('improves', 'performance')-0.25\n",
            "('performance', 'learning')-0.047619047619047616\n",
            "('learning', 'the')-0.038461538461538464\n",
            "('the', 'system')-0.07142857142857142\n",
            "('system', 'processes')-0.2\n",
            "('processes', 'short')-0.5\n",
            "('short', 'english')-0.5\n",
            "('english', 'narratives')-1.0\n",
            "('narratives', 'able')-0.5\n",
            "('able', 'acquire')-0.5\n",
            "('acquire', 'single')-0.3333333333333333\n",
            "('single', 'narrative')-1.0\n",
            "('narrative', 'new')-0.16666666666666666\n",
            "('new', 'schema')-1.0\n",
            "('schema', 'stereotypical')-1.0\n",
            "('stereotypical', 'set')-0.5\n",
            "('set', 'actions')-1.0\n",
            "('actions', 'during')-0.5\n",
            "('during', 'understanding')-0.16666666666666666\n",
            "('understanding', 'process')-0.125\n",
            "('process', 'system')-0.07142857142857142\n",
            "('system', 'attempts')-1.0\n",
            "('attempts', 'we')-0.041666666666666664\n",
            "('we', 'classify')-1.0\n",
            "('classify', 'review')-0.5\n",
            "('review', 'current')-0.14285714285714285\n",
            "('current', 'approaches')-0.16666666666666666\n",
            "('approaches', 'software')-0.25\n",
            "('software', 'infrastructure')-0.5\n",
            "('infrastructure', 'research')-0.08333333333333333\n",
            "('research', 'development')-0.1\n",
            "('development', 'delivery')-1.0\n",
            "('delivery', 'nlp')-0.022222222222222223\n",
            "('nlp', 'systems')-0.0625\n",
            "('the', 'task')-0.2\n",
            "('task', 'confidence')-0.25\n",
            "('confidence', 'measures')-0.6666666666666666\n",
            "('measures', 'practical')-0.3333333333333333\n",
            "('practical', 'solution')-0.5\n",
            "('solution', 'improving')-1.0\n",
            "('improving', 'usefulness')-1.0\n",
            "('usefulness', 'natural')-0.010416666666666666\n",
            "('applications', 'confidence')-0.25\n",
            "('confidence', 'estimation')-1.0\n",
            "('estimation', 'generic')-0.3333333333333333\n",
            "('generic', 'machine')-0.058823529411764705\n",
            "('learning', 'approach')-0.09090909090909091\n",
            "('approach', 'deriving')-1.0\n",
            "('deriving', 'confidence')-0.25\n",
            "('measures', 'we')-0.041666666666666664\n",
            "('we', 'give')-1.0\n",
            "('give', 'overview')-0.5\n",
            "('overview', 'application')-0.2\n",
            "('application', 'confidence')-0.25\n",
            "('estimation', 'various')-0.125\n",
            "('various', 'fields')-0.3333333333333333\n",
            "('fields', 'lex')-0.3333333333333333\n",
            "('lex', 'sign')-0.75\n",
            "('sign', 'sense')-0.25\n",
            "('sense', 'id')-1.0\n",
            "('id', 'sense')-0.3333333333333333\n",
            "('id', 'dictionary')-0.3333333333333333\n",
            "('dictionary', 'ldoce')-0.5\n",
            "('ldoce', 'lex')-0.3333333333333333\n",
            "('id', 'ldb')-1.0\n",
            "('ldb', 'entry')-1.0\n",
            "('entry', 'lex')-0.3333333333333333\n",
            "('sense', 'when')-1.0\n",
            "('when', 'loaded')-1.0\n",
            "('loaded', 'lkb')-0.25\n",
            "('lkb', 'expanded')-0.5\n",
            "('expanded', 'fully')-1.0\n",
            "('fully', 'fledged')-1.0\n",
            "('fledged', 'representation')-0.5\n",
            "('representation', 'transitive')-1.0\n",
            "('transitive', 'use')-0.08333333333333333\n",
            "('use', 'experience')-0.5\n",
            "('experience', 'integrating')-0.5\n",
            "('integrating', 'word')-0.1111111111111111\n",
            "('word', 'specific')-0.5\n",
            "('specific', 'information')-0.045454545454545456\n",
            "('information', 'provided')-1.0\n",
            "('provided', 'information')-0.045454545454545456\n",
            "('information', 'encoded')-1.0\n",
            "('encoded', 'lkb')-0.25\n",
            "('lkb', 'type')-0.3333333333333333\n",
            "('type', 'strict')-1.0\n",
            "('strict', 'trans')-1.0\n",
            "('trans', 'sign')-0.25\n",
            "('sign', 'thus')-0.5\n",
            "('thus', 'although')-0.5\n",
            "('although', 'neither')-1.0\n",
            "('neither', 'ldoce')-0.5\n",
            "('ldoce', 'llce')-1.0\n",
            "('llce', 'earlier')-1.0\n",
            "('earlier', 'subcategorised')-1.0\n",
            "('subcategorised', 'lexicon')-1.0\n",
            "('lexicon', 'contain')-1.0\n",
            "('contain', 'information')-0.045454545454545456\n",
            "('information', 'psychological')-1.0\n",
            "('psychological', 'verbs')-1.0\n",
            "('verbs', 'defined')-1.0\n",
            "('defined', 'sanfilippo')-1.0\n",
            "('sanfilippo', 'aposs')-1.0\n",
            "('aposs', 'type')-0.3333333333333333\n",
            "('type', 'system')-0.14285714285714285\n",
            "('system', 'using')-0.09090909090909091\n",
            "('using', 'conjunction')-1.0\n",
            "('conjunction', 'information')-0.045454545454545456\n",
            "('information', 'available')-1.0\n",
            "('available', 'three')-0.3333333333333333\n",
            "('three', 'proved')-1.0\n",
            "('proved', 'possible')-0.5\n",
            "('possible', 'effectively')-0.3333333333333333\n",
            "('effectively', 'enrich')-1.0\n",
            "('enrich', 'information')-0.045454545454545456\n",
            "('information', 'time')-0.5\n",
            "('time', 'mapping')-1.0\n",
            "('mapping', 'formal')-1.0\n",
            "('formal', 'representation')-0.5\n",
            "('representation', 'towards')-1.0\n",
            "('towards', 'multilingual')-0.5\n",
            "('multilingual', 'lkb')-0.25\n",
            "('lkb', 'a')-0.25\n",
            "('a', 'goal')-0.3333333333333333\n",
            "('goal', 'acquilex')-1.0\n",
            "('acquilex', 'demonstrate')-0.5\n",
            "('demonstrate', 'lkb')-0.25\n",
            "('lkb', 'produced')-0.5\n",
            "('produced', 'usefully')-1.0\n",
            "('usefully', 'exploits')-1.0\n",
            "('exploits', 'various')-0.125\n",
            "('various', 'mrd')-1.0\n",
            "('mrd', 'sources')-1.0\n",
            "('sources', 'integrates')-1.0\n",
            "('integrates', 'multilingual')-0.5\n",
            "('multilingual', 'information')-0.045454545454545456\n",
            "('information', 'the')-0.038461538461538464\n",
            "('the', 'use')-0.08333333333333333\n",
            "('use', 'common')-0.5\n",
            "('common', 'lrl')-1.0\n",
            "('lrl', 'common')-0.5\n",
            "('common', 'type')-0.3333333333333333\n",
            "('system', 'makes')-0.5\n",
            "('makes', 'possi')-1.0\n",
            "('possi', 'we')-0.041666666666666664\n",
            "('describe', 'design')-0.25\n",
            "('design', 'use')-0.08333333333333333\n",
            "('use', 'stanford')-1.0\n",
            "('stanford', 'corenlp')-1.0\n",
            "('corenlp', 'toolkit')-0.5\n",
            "('toolkit', 'extensible')-1.0\n",
            "('extensible', 'pipeline')-1.0\n",
            "('pipeline', 'provides')-0.5\n",
            "('provides', 'core')-1.0\n",
            "('core', 'natural')-0.010416666666666666\n",
            "('natural', 'lan')-1.0\n",
            "('lan', 'guage')-1.0\n",
            "('guage', 'analysis')-0.09090909090909091\n",
            "('analysis', 'this')-0.034482758620689655\n",
            "('this', 'toolkit')-0.5\n",
            "('toolkit', 'quite')-1.0\n",
            "('quite', 'widely')-1.0\n",
            "('widely', 'used')-0.09090909090909091\n",
            "('used', 'research')-0.08333333333333333\n",
            "('research', 'nlp')-0.022222222222222223\n",
            "('nlp', 'community')-0.5\n",
            "('community', 'also')-0.2\n",
            "('also', 'among')-0.3333333333333333\n",
            "('among', 'commercial')-1.0\n",
            "('commercial', 'govern')-1.0\n",
            "('govern', 'ment')-1.0\n",
            "('ment', 'users')-1.0\n",
            "('users', 'open')-1.0\n",
            "('open', 'source')-1.0\n",
            "('source', 'nlp')-0.022222222222222223\n",
            "('nlp', 'technol')-1.0\n",
            "('technol', 'ogy')-1.0\n",
            "('ogy', 'we')-0.041666666666666664\n",
            "('we', 'suggest')-1.0\n",
            "('suggest', 'gaussian')-1.0\n",
            "('gaussian', 'processes')-0.2\n",
            "('processes', 'gps')-1.0\n",
            "('gps', 'powerful')-0.5\n",
            "('powerful', 'mod')-1.0\n",
            "('mod', 'elling')-1.0\n",
            "('elling', 'framework')-0.5\n",
            "('framework', 'incorporating')-1.0\n",
            "('incorporating', 'kernels')-1.0\n",
            "('kernels', 'bayesian')-1.0\n",
            "('bayesian', 'inference')-1.0\n",
            "('inference', 'recognised')-1.0\n",
            "('recognised', 'state')-0.125\n",
            "('art', 'many')-0.2\n",
            "('many', 'machine')-0.058823529411764705\n",
            "('learning', 'tasks')-0.0625\n",
            "('tasks', 'a')-0.25\n",
            "('a', 'fundamental')-0.5\n",
            "('fundamental', 'issue')-0.3333333333333333\n",
            "('issue', 'natural')-0.010416666666666666\n",
            "('processing', 'prerequisite')-1.0\n",
            "('prerequisite', 'enormous')-1.0\n",
            "('enormous', 'quantity')-1.0\n",
            "('quantity', 'preprogrammed')-1.0\n",
            "('preprogrammed', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'concerning')-0.5\n",
            "('concerning', 'language')-0.009174311926605505\n",
            "('domain', 'examination')-1.0\n",
            "('examination', 'manual')-0.3333333333333333\n",
            "('manual', 'acquisition')-0.3333333333333333\n",
            "('acquisition', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'tedious')-1.0\n",
            "('tedious', 'error')-1.0\n",
            "('error', 'prone')-1.0\n",
            "('prone', 'development')-0.1\n",
            "('development', 'automated')-0.2\n",
            "('automated', 'acquisition')-0.3333333333333333\n",
            "('acquisition', 'supports')-1.0\n",
            "('supports', 'sophisticated')-0.5\n",
            "('sophisticated', 'natural')-0.010416666666666666\n",
            "('processing', 'significantly')-1.0\n",
            "('significantly', 'simplifying')-1.0\n",
            "('simplifying', 'interface')-0.5\n",
            "('interface', 'domain')-0.25\n",
            "('domain', 'specific')-0.5\n",
            "('specific', 'knowledge')-0.08333333333333333\n",
            "('knowledge', 'general')-0.3333333333333333\n",
            "('general', 'linguis')-1.0\n",
            "('linguis', 'tic')-1.0\n",
            "('tic', 'resources')-0.2\n",
            "('resources', 'this')-0.034482758620689655\n",
            "('paper', 'presents')-0.6666666666666666\n",
            "('presents', 'results')-0.14285714285714285\n",
            "('results', 'experiences')-1.0\n",
            "('experiences', 'designing')-1.0\n",
            "('designing', 'using')-0.09090909090909091\n",
            "('using', 'upper')-1.0\n",
            "('upper', 'model')-0.14285714285714285\n",
            "('model', 'variety')-0.5\n",
            "('variety', 'applications')-0.08333333333333333\n",
            "('applications', 'past')-0.5\n",
            "('years', 'neighboring')-1.0\n",
            "('neighboring', 'map')-1.0\n",
            "('map', 'nodes')-0.5\n",
            "('nodes', 'nodes')-0.5\n",
            "('nodes', 'may')-0.3333333333333333\n",
            "('may', 'thus')-0.5\n",
            "('thus', 'viewed')-1.0\n",
            "('viewed', 'word')-0.1111111111111111\n",
            "('word', 'categories')-1.0\n",
            "('categories', 'although')-0.5\n",
            "('although', 'priori')-1.0\n",
            "('priori', 'information')-0.045454545454545456\n",
            "('information', 'classes')-0.3333333333333333\n",
            "('classes', 'given')-0.2\n",
            "('given', 'self')-1.0\n",
            "('self', 'organizing')-1.0\n",
            "('organizing', 'process')-0.125\n",
            "('process', 'model')-0.14285714285714285\n",
            "('model', 'word')-0.1111111111111111\n",
            "('word', 'classes')-0.3333333333333333\n",
            "('classes', 'emerges')-1.0\n",
            "('emerges', 'the')-0.038461538461538464\n",
            "('the', 'central')-1.0\n",
            "('central', 'topic')-1.0\n",
            "('topic', 'thesis')-0.5\n",
            "('thesis', 'use')-0.08333333333333333\n",
            "('use', 'som')-1.0\n",
            "('som', 'natural')-0.010416666666666666\n",
            "('the', 'approach')-0.09090909090909091\n",
            "('presents', 'workbench')-0.5\n",
            "('workbench', 'built')-0.5\n",
            "('built', 'priberam')-1.0\n",
            "('priberam', 'inform')-0.3333333333333333\n",
            "('inform', 'tica')-1.0\n",
            "('tica', 'development')-0.1\n",
            "('development', 'company')-1.0\n",
            "('company', 'natural')-0.010416666666666666\n",
            "('processing', 'technology')-0.3333333333333333\n",
            "('technology', 'this')-0.034482758620689655\n",
            "('this', 'workbench')-0.5\n",
            "('workbench', 'includes')-0.3333333333333333\n",
            "('includes', 'set')-0.5\n",
            "('set', 'linguistic')-0.14285714285714285\n",
            "('linguistic', 'resources')-0.2\n",
            "('resources', 'software')-0.25\n",
            "('software', 'tools')-0.3333333333333333\n",
            "('tools', 'applied')-0.2\n",
            "('applied', 'considerable')-1.0\n",
            "('considerable', 'number')-0.2\n",
            "('number', 'practical')-0.3333333333333333\n",
            "('practical', 'purposes')-0.5\n",
            "('purposes', 'covering')-0.5\n",
            "('covering', 'abstract')-0.05\n",
            "('nlp', 'effective')-0.25\n",
            "('effective', 'approach')-0.09090909090909091\n",
            "('approach', 'bringing')-1.0\n",
            "('bringing', 'improvement')-0.25\n",
            "('improvement', 'educational')-0.5\n",
            "('educational', 'setting')-1.0\n",
            "('setting', 'implementing')-1.0\n",
            "('implementing', 'nlp')-0.022222222222222223\n",
            "('nlp', 'involves')-0.5\n",
            "('involves', 'initiating')-1.0\n",
            "('initiating', 'process')-0.125\n",
            "('process', 'learning')-0.047619047619047616\n",
            "('learning', 'natural')-0.010416666666666666\n",
            "('natural', 'acquisition')-0.3333333333333333\n",
            "('acquisition', 'educational')-0.5\n",
            "('educational', 'systems')-0.0625\n",
            "('systems', 'it')-0.2\n",
            "('it', 'based')-0.07692307692307693\n",
            "('based', 'effective')-0.25\n",
            "('effective', 'approaches')-0.16666666666666666\n",
            "('approaches', 'providing')-0.5\n",
            "('providing', 'solution')-0.5\n",
            "('solution', 'abstract')-0.05\n",
            "('abstract', 'after')-0.5\n",
            "('after', 'twenty')-1.0\n",
            "('twenty', 'years')-0.125\n",
            "('years', 'disfavor')-1.0\n",
            "('disfavor', 'technology')-0.3333333333333333\n",
            "('technology', 'returned')-1.0\n",
            "('returned', 'imitates')-1.0\n",
            "('imitates', 'processes')-0.2\n",
            "('processes', 'brain')-1.0\n",
            "('brain', 'natural')-0.010416666666666666\n",
            "('language', 'experiments')-0.5\n",
            "('experiments', 'sejnowski')-1.0\n",
            "('sejnowski', 'rosenberg')-1.0\n",
            "('rosenberg', 'demonstrate')-0.5\n",
            "('demonstrate', 'neural')-0.3333333333333333\n",
            "('network', 'computing')-1.0\n",
            "('computing', 'architecture')-0.25\n",
            "('architecture', 'learn')-0.5\n",
            "('learn', 'actual')-1.0\n",
            "('actual', 'spoken')-0.25\n",
            "('spoken', 'language')-0.009174311926605505\n",
            "('language', 'observe')-1.0\n",
            "('observe', 'rules')-0.5\n",
            "('rules', 'pronunciation')-1.0\n",
            "('pronunciation', 'text')-0.09090909090909091\n",
            "('text', 'statistics')-1.0\n",
            "('statistics', 'frequently')-1.0\n",
            "('frequently', 'used')-0.09090909090909091\n",
            "('used', 'stylometry')-1.0\n",
            "('stylometry', 'cryptography')-1.0\n",
            "('cryptography', 'studies')-0.5\n",
            "('studies', 'in')-0.058823529411764705\n",
            "('paper', 'text')-0.09090909090909091\n",
            "('statistics', 'tools')-0.3333333333333333\n",
            "('tools', 'developed')-0.2\n",
            "('developed', 'iso')-1.0\n",
            "('iso', 'prolog')-0.5\n",
            "('prolog', 'natural')-0.010416666666666666\n",
            "('processing', 'details')-1.0\n",
            "('details', 'given')-0.2\n",
            "('given', 'usage')-1.0\n",
            "('usage', 'user')-0.5\n",
            "('user', 'callable')-1.0\n",
            "('callable', 'predicates')-1.0\n",
            "('predicates', 'logic')-0.16666666666666666\n",
            "('logic', 'limitations')-1.0\n",
            "('limitations', 'program')-0.3333333333333333\n",
            "('program', 'also')-0.2\n",
            "('also', 'discussed')-1.0\n",
            "('discussed', 'we')-0.041666666666666664\n",
            "('we', 'summarize')-1.0\n",
            "('summarize', 'experience')-0.5\n",
            "('experience', 'using')-0.09090909090909091\n",
            "('using', 'framenet')-0.5\n",
            "('framenet', 'two')-0.3333333333333333\n",
            "('two', 'rather')-0.3333333333333333\n",
            "('rather', 'different')-0.25\n",
            "('different', 'projects')-0.5\n",
            "('projects', 'natural')-0.010416666666666666\n",
            "('nlp', 'we')-0.041666666666666664\n",
            "('we', 'conclude')-1.0\n",
            "('conclude', 'nlp')-0.022222222222222223\n",
            "('nlp', 'benefit')-0.5\n",
            "('benefit', 'framenet')-0.5\n",
            "('framenet', 'different')-0.25\n",
            "('different', 'ways')-1.0\n",
            "('ways', 'sketch')-1.0\n",
            "('sketch', 'problems')-0.3333333333333333\n",
            "('problems', 'need')-0.16666666666666666\n",
            "('need', 'overcome')-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2wHn3b1RJEn"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyraDbm9spjo",
        "outputId": "c09d1656-d18d-4e87-d5cc-95f88e55ae5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5aHSDtota61"
      },
      "source": [
        "def get_sentence():\n",
        "  s = ''\n",
        "  for  i in df['Lower casing']:\n",
        "    s = s + i\n",
        "  return s"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox-ik7U4LUcP",
        "outputId": "0a472b5f-1929-4812-9902-09cf3ff82338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "source": [
        "# 1.3\n",
        "# Extracting nouns\n",
        "freq = []\n",
        "phrases = []\n",
        "s = get_sentence()\n",
        "for i in df['Lower casing']:\n",
        "  for values in TextBlob(i).noun_phrases:\n",
        "    phrases.append(values)\n",
        "for phrase in phrases:\n",
        "  f = []\n",
        "  for j in df['Lower casing']:\n",
        "    percentage = j.count(phrase)/s.count(phrase)\n",
        "    f.append(percentage)\n",
        "  freq.append(f)\n",
        "np_df = pd.DataFrame(freq).T\n",
        "np_df.columns = phrases\n",
        "np_df"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>describe method</th>\n",
              "      <th>maximum entropy</th>\n",
              "      <th>present maximum likelihood approach</th>\n",
              "      <th>maximum entropy models describe</th>\n",
              "      <th>natural language processing</th>\n",
              "      <th>conditional random fields</th>\n",
              "      <th>natural language processing terms conditions terms conditions copyright</th>\n",
              "      <th>minerva access</th>\n",
              "      <th>paper addresses issue cooperation linguistics</th>\n",
              "      <th>natural language processing nlp general linguistics machine translation mt</th>\n",
              "      <th>direction cooperation</th>\n",
              "      <th>applications linguistics nlp</th>\n",
              "      <th>natural language processing applications description logics</th>\n",
              "      <th>encode knowledge base syntactic semantic pragmatic elements</th>\n",
              "      <th>drive semantic interpretation</th>\n",
              "      <th>natural language generation processes</th>\n",
              "      <th>description logics</th>\n",
              "      <th>neural network architecture learning algorithm</th>\n",
              "      <th>various natural language processing tasks</th>\n",
              "      <th>part speech</th>\n",
              "      <th>entity recognition semantic role</th>\n",
              "      <th>natural language processing</th>\n",
              "      <th>natural language processing</th>\n",
              "      <th>broad narrow senses</th>\n",
              "      <th>broad sense</th>\n",
              "      <th>processing issues levels</th>\n",
              "      <th>natural language understanding</th>\n",
              "      <th>speech recognition syntactic semantic analysis sentences</th>\n",
              "      <th>robots interact humans</th>\n",
              "      <th>natural language need responsive way humans</th>\n",
              "      <th>language situations</th>\n",
              "      <th>natural language processing system robots performs incremental semantic interpretation</th>\n",
              "      <th>natural languages languages</th>\n",
              "      <th>point languages</th>\n",
              "      <th>natural language processing collection techniques</th>\n",
              "      <th>abstract ambiguity</th>\n",
              "      <th>natural languages</th>\n",
              "      <th>ambiguous computers</th>\n",
              "      <th>language way people</th>\n",
              "      <th>natural language processing nlp</th>\n",
              "      <th>...</th>\n",
              "      <th>interface domain</th>\n",
              "      <th>specific knowledge general linguis tic resources</th>\n",
              "      <th>results experiences</th>\n",
              "      <th>upper model variety applications past years</th>\n",
              "      <th>map nodes nodes</th>\n",
              "      <th>word categories</th>\n",
              "      <th>priori information classes</th>\n",
              "      <th>process model word classes</th>\n",
              "      <th>central topic thesis use som</th>\n",
              "      <th>natural language processing</th>\n",
              "      <th>tica development company</th>\n",
              "      <th>natural language processing technology</th>\n",
              "      <th>linguistic resources software tools</th>\n",
              "      <th>considerable number</th>\n",
              "      <th>practical purposes</th>\n",
              "      <th>natural language processing nlp</th>\n",
              "      <th>effective approach</th>\n",
              "      <th>educational setting</th>\n",
              "      <th>nlp involves</th>\n",
              "      <th>process learning</th>\n",
              "      <th>natural acquisition</th>\n",
              "      <th>educational systems</th>\n",
              "      <th>effective approaches</th>\n",
              "      <th>years disfavor technology</th>\n",
              "      <th>imitates processes brain</th>\n",
              "      <th>natural language experiments sejnowski rosenberg</th>\n",
              "      <th>neural network</th>\n",
              "      <th>language observe rules pronunciation</th>\n",
              "      <th>text statistics</th>\n",
              "      <th>stylometry cryptography studies</th>\n",
              "      <th>paper text statistics tools</th>\n",
              "      <th>iso prolog</th>\n",
              "      <th>natural language processing details</th>\n",
              "      <th>usage user</th>\n",
              "      <th>callable predicates logic limitations</th>\n",
              "      <th>summarize experience</th>\n",
              "      <th>different projects</th>\n",
              "      <th>natural language processing nlp</th>\n",
              "      <th>conclude nlp benefit framenet</th>\n",
              "      <th>different ways sketch problems need</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013158</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    describe method  ...  different ways sketch problems need\n",
              "0               0.0  ...                                  0.0\n",
              "1               1.0  ...                                  0.0\n",
              "2               0.0  ...                                  0.0\n",
              "3               0.0  ...                                  0.0\n",
              "4               0.0  ...                                  0.0\n",
              "..              ...  ...                                  ...\n",
              "95              0.0  ...                                  0.0\n",
              "96              0.0  ...                                  0.0\n",
              "97              0.0  ...                                  0.0\n",
              "98              0.0  ...                                  0.0\n",
              "99              0.0  ...                                  1.0\n",
              "\n",
              "[100 rows x 501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4O2qb1Fl3kL"
      },
      "source": [
        "\n",
        "# **Question 2: Undersand TF-IDF and Document representation**\n",
        "\n",
        "\n",
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w71w6d2NAzsx"
      },
      "source": [
        "import math\n",
        "sentences = final_df[\"Lower casing\"].values.tolist()\n",
        "words = set([j for i in sentences for j in i.split(\" \")])\n",
        "tf_idf_df = pd.DataFrame(words, columns=[\"words\"])\n",
        "count=1\n",
        "\n",
        "def tf_idf(x,sentence):\n",
        "  value = sentence.count(x)\n",
        "  size = len(sentence.split(\" \"))\n",
        "  if(value != 0):\n",
        "    return (value/size)*(math.log(size/value, 10))\n",
        "  else:\n",
        "    return 0\n",
        "    \n",
        "for sentence in sentences:\n",
        "  tf_idf_df[\"Review\"+str(count)] = tf_idf_df[\"words\"].apply(lambda x: tf_idf(x,sentence))\n",
        "  count=count+1"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7pCqVjeBvXe",
        "outputId": "8f383814-e582-4bfe-b1e0-025cb393a353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "tf_idf_df"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>Review1</th>\n",
              "      <th>Review2</th>\n",
              "      <th>Review3</th>\n",
              "      <th>Review4</th>\n",
              "      <th>Review5</th>\n",
              "      <th>Review6</th>\n",
              "      <th>Review7</th>\n",
              "      <th>Review8</th>\n",
              "      <th>Review9</th>\n",
              "      <th>Review10</th>\n",
              "      <th>Review11</th>\n",
              "      <th>Review12</th>\n",
              "      <th>Review13</th>\n",
              "      <th>Review14</th>\n",
              "      <th>Review15</th>\n",
              "      <th>Review16</th>\n",
              "      <th>Review17</th>\n",
              "      <th>Review18</th>\n",
              "      <th>Review19</th>\n",
              "      <th>Review20</th>\n",
              "      <th>Review21</th>\n",
              "      <th>Review22</th>\n",
              "      <th>Review23</th>\n",
              "      <th>Review24</th>\n",
              "      <th>Review25</th>\n",
              "      <th>Review26</th>\n",
              "      <th>Review27</th>\n",
              "      <th>Review28</th>\n",
              "      <th>Review29</th>\n",
              "      <th>Review30</th>\n",
              "      <th>Review31</th>\n",
              "      <th>Review32</th>\n",
              "      <th>Review33</th>\n",
              "      <th>Review34</th>\n",
              "      <th>Review35</th>\n",
              "      <th>Review36</th>\n",
              "      <th>Review37</th>\n",
              "      <th>Review38</th>\n",
              "      <th>Review39</th>\n",
              "      <th>...</th>\n",
              "      <th>Review61</th>\n",
              "      <th>Review62</th>\n",
              "      <th>Review63</th>\n",
              "      <th>Review64</th>\n",
              "      <th>Review65</th>\n",
              "      <th>Review66</th>\n",
              "      <th>Review67</th>\n",
              "      <th>Review68</th>\n",
              "      <th>Review69</th>\n",
              "      <th>Review70</th>\n",
              "      <th>Review71</th>\n",
              "      <th>Review72</th>\n",
              "      <th>Review73</th>\n",
              "      <th>Review74</th>\n",
              "      <th>Review75</th>\n",
              "      <th>Review76</th>\n",
              "      <th>Review77</th>\n",
              "      <th>Review78</th>\n",
              "      <th>Review79</th>\n",
              "      <th>Review80</th>\n",
              "      <th>Review81</th>\n",
              "      <th>Review82</th>\n",
              "      <th>Review83</th>\n",
              "      <th>Review84</th>\n",
              "      <th>Review85</th>\n",
              "      <th>Review86</th>\n",
              "      <th>Review87</th>\n",
              "      <th>Review88</th>\n",
              "      <th>Review89</th>\n",
              "      <th>Review90</th>\n",
              "      <th>Review91</th>\n",
              "      <th>Review92</th>\n",
              "      <th>Review93</th>\n",
              "      <th>Review94</th>\n",
              "      <th>Review95</th>\n",
              "      <th>Review96</th>\n",
              "      <th>Review97</th>\n",
              "      <th>Review98</th>\n",
              "      <th>Review99</th>\n",
              "      <th>Review100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>introduced</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035577</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.062963</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>compound</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050428</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unification</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>educational</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083728</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>central</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047036</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>research</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103933</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051684</td>\n",
              "      <td>0.051684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.050428</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.081866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>agents</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>start</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>eliminate</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.051684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>written</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1118 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            words  Review1  Review2  ...  Review98  Review99  Review100\n",
              "0      introduced      0.0      0.0  ...       0.0       0.0        0.0\n",
              "1        compound      0.0      0.0  ...       0.0       0.0        0.0\n",
              "2     unification      0.0      0.0  ...       0.0       0.0        0.0\n",
              "3     educational      0.0      0.0  ...       0.0       0.0        0.0\n",
              "4         central      0.0      0.0  ...       0.0       0.0        0.0\n",
              "...           ...      ...      ...  ...       ...       ...        ...\n",
              "1113     research      0.0      0.0  ...       0.0       0.0        0.0\n",
              "1114       agents      0.0      0.0  ...       0.0       0.0        0.0\n",
              "1115        start      0.0      0.0  ...       0.0       0.0        0.0\n",
              "1116    eliminate      0.0      0.0  ...       0.0       0.0        0.0\n",
              "1117      written      0.0      0.0  ...       0.0       0.0        0.0\n",
              "\n",
              "[1118 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhbv-yjLwu0W"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "X_Value = \"text directly (rather than e.g. titles and abstracts), and suggests appropriate approaches to doing this, with a focus on the role of natural language processing. The paper also comments on possible connections with data and knowledge retrieval, and concludes by emphasizing the importance of rigorous\"\n",
        "X_after_tokens = word_tokenize(X_Value)\n",
        "stop = stopwords.words('english') \n",
        "X_after_stop_words = {word for word in X_after_tokens if not word in stop}"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zieuo0WTxcin"
      },
      "source": [
        "def cosine_value(X_after_stop_words, Y_after_stop_words):\n",
        "  cosine = 0\n",
        "  list1 = []\n",
        "  list2 = []\n",
        "  rvector = X_after_stop_words.union(Y_after_stop_words)  \n",
        "  for w in rvector: \n",
        "    if w in X_after_stop_words: list1.append(1)\n",
        "    else: list1.append(0) \n",
        "    if w in X_after_stop_words: list2.append(1) \n",
        "    else: list2.append(0) \n",
        "  for i in range(len(rvector)): \n",
        "    cosine += list1[i]*list2[i] \n",
        "  cv = cosine / float((sum(list1)*sum(list2))**0.5)\n",
        "  return cv"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMd8pvuImEa2"
      },
      "source": [
        "cosine_similarity = []\n",
        "for i in df['Lower casing']:\n",
        "  Y_after_tokens = word_tokenize(i)\n",
        "  Y_after_stop_words = {word for word in Y_after_tokens if not word in stop}\n",
        "  cosine_similarity.append(cosine_value(X_after_stop_words, Y_after_stop_words))"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ihYwKdOyZH9",
        "outputId": "74b1cfa7-575a-4981-bc49-eb1ed0ec83bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "cs_df = pd.DataFrame(list(zip(df['Lower casing'], cosine_similarity)), columns = ['Abstracts', 'CS'])\n",
        "cs_df"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstracts</th>\n",
              "      <th>CS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>abstract found</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe method statistical modeling based max...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>scaling conditional random fields natural lang...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the paper addresses issue cooperation linguist...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in natural language processing applications de...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>this paper presents workbench built priberam i...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>abstract natural language processing nlp effec...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>abstract after twenty years disfavor technolog...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>text statistics frequently used stylometry cry...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>we summarize experience using framenet two rat...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Abstracts   CS\n",
              "0                                      abstract found  1.0\n",
              "1   describe method statistical modeling based max...  1.0\n",
              "2   scaling conditional random fields natural lang...  1.0\n",
              "3   the paper addresses issue cooperation linguist...  1.0\n",
              "4   in natural language processing applications de...  1.0\n",
              "..                                                ...  ...\n",
              "95  this paper presents workbench built priberam i...  1.0\n",
              "96  abstract natural language processing nlp effec...  1.0\n",
              "97  abstract after twenty years disfavor technolog...  1.0\n",
              "98  text statistics frequently used stylometry cry...  1.0\n",
              "99  we summarize experience using framenet two rat...  1.0\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWiAP8AXmMLq"
      },
      "source": [
        "\n",
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**\n",
        "\n",
        "\n",
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryyyf5GvmTCF",
        "outputId": "4b9c1a41-2fed-49ef-ce60-361bcfe1f1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df = pd.DataFrame((abstract_text), columns =['Abstract'])\n",
        "df.to_csv('Abstracts for Sentiment Analysis.csv')\n",
        "df"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abstract not found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>describe a method for statistical modeling bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Scaling conditional random fields for natural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The paper addresses the issue of cooperation b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>This paper presents a workbench built by Pribe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Abstract—Natural Language Processing (NLP) is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ABSTRACT: After twenty years of disfavor, a te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Text statistics are frequently used in stylome...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>We summarize our experience using FrameNet in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Abstract\n",
              "0                                  Abstract not found\n",
              "1   describe a method for statistical modeling bas...\n",
              "2   Scaling conditional random fields for natural ...\n",
              "3   The paper addresses the issue of cooperation b...\n",
              "4   In most natural language processing applicatio...\n",
              "..                                                ...\n",
              "95  This paper presents a workbench built by Pribe...\n",
              "96  Abstract—Natural Language Processing (NLP) is ...\n",
              "97  ABSTRACT: After twenty years of disfavor, a te...\n",
              "98  Text statistics are frequently used in stylome...\n",
              "99  We summarize our experience using FrameNet in ...\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_F97DSOE7Pk"
      },
      "source": [
        "File link - \n",
        "\n",
        "https://github.com/DurgaBhavana/5731Submissions/blob/master/Abstracts%20for%20Sentiment%20Analysis.csv"
      ]
    }
  ]
}